{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "428d10f1",
   "metadata": {},
   "source": [
    "## Experiment Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da08a5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_TOKENIZATION=False\n",
    "TINY_DATA_LOAD=True # If True, load tiny data for testing\n",
    "TRAIN_MODEL=True\n",
    "\n",
    "DATA_PREFIX='expt_1'\n",
    "# DATA_PREFIX='tiny_sample'\n",
    "MODEL_EXPT_NAME=\"multitok_model_1\"\n",
    "\n",
    "TRAIN_DATA_PATH=f'train-model/{DATA_PREFIX}'\n",
    "MODEL_PATH=f'train-model/{DATA_PREFIX}/{MODEL_EXPT_NAME}'\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"eval_interval\": 300,\n",
    "    \"max_iters\": 60000, \n",
    "    \"H\": 32, # per head dimension size\n",
    "    \"B\": 64, # batch size\n",
    "    \"T\": 256, # Sequence length\n",
    "    \"C\": 256, # model size\n",
    "    \"feedforward_factor\": 3,\n",
    "    \"n_heads\": 8,\n",
    "    \"dropout\": 0.0,\n",
    "    \"l2_penalty\": 0.0,\n",
    "    \"n_layers\": 12,\n",
    "    \"vocab_size\": 2**13,\n",
    "    # \"git_hash\": os.popen(\"git rev-parse HEAD\").read().strip()\n",
    "}\n",
    "\n",
    "for k,v in config.items():\n",
    "    locals()[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7726f1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lingua in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.15.0)\n",
      "Requirement already satisfied: polib>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lingua) (1.2.0)\n",
      "Requirement already satisfied: click>=8.0.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lingua) (8.1.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: multi-tokenizer in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.1.4)\n",
      "Requirement already satisfied: lingua-language-detector<3.0.0,>=2.0.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from multi-tokenizer) (2.0.2)\n",
      "Requirement already satisfied: tokenizers<0.20.0,>=0.19.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from multi-tokenizer) (0.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (0.23.4)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (2024.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (6.0.1)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (2024.6.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: ipython-unittest in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.3.2)\n",
      "Requirement already satisfied: ipython in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython-unittest) (8.17.2)\n",
      "Requirement already satisfied: decorator in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ipython-unittest) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ipython-unittest) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ipython-unittest) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ipython-unittest) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ipython-unittest) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ipython-unittest) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ipython-unittest) (5.14.3)\n",
      "Requirement already satisfied: exceptiongroup in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ipython-unittest) (1.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ipython-unittest) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jedi>=0.16->ipython->ipython-unittest) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pexpect>4.3->ipython->ipython-unittest) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython->ipython-unittest) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from stack-data->ipython->ipython-unittest) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from stack-data->ipython->ipython-unittest) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from stack-data->ipython->ipython-unittest) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython->ipython-unittest) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install lingua \n",
    "!pip install multi-tokenizer\n",
    "!pip install ipython-unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa17e9b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feadb260-56af-4aa5-951a-55d3a04d157e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from itertools import chain\n",
    "import tokenizers\n",
    "    \n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from itertools import islice\n",
    "from typing import List\n",
    "\n",
    "%load_ext ipython_unittest\n",
    "\n",
    "from multi_tokenizer import LanguageDetector\n",
    "from lingua import DetectionResult, Language\n",
    "\n",
    "\n",
    "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_default_device(device)\n",
    "# assert device == 'cuda', \"This notebook is not optimized for CPU\"\n",
    "if device == 'cpu' and TRAIN_MODEL:\n",
    "    print(\"Warning: Training on CPU will be slow\")\n",
    "    assert False\n",
    "\n",
    "if 'train-model' not in sys.path: sys.path.append('train-model')\n",
    "import data_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e920f455",
   "metadata": {},
   "source": [
    "## Tokenization preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14f37cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "END_TOKEN = '[END]'\n",
    "\n",
    "def delimited_stories_iterator(text):\n",
    "    start = 0\n",
    "    last_match_end = text.find(END_TOKEN)\n",
    "    while last_match_end != -1:\n",
    "        yield text[start:last_match_end + len(END_TOKEN)]\n",
    "        start = last_match_end + len(END_TOKEN)\n",
    "        last_match_end = text.find(END_TOKEN, start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91730595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/unittest.status+json": {
       "color": "yellow",
       "message": "",
       "previous": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/unittest.status+json": {
       "color": "lightgreen",
       "message": ".\n----------------------------------------------------------------------\nRan 1 test in 0.000s\n\nOK\n",
       "previous": 0
      },
      "text/plain": [
       "Success"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%unittest_testcase\n",
    "def test_delimited_stories_iterator(self):\n",
    "    test_input = \"\"\"[PROMPT] español [USER] Un niñ [END]\n",
    "[PROMPT] español [USER] Hola [END]\"\"\"\n",
    "    delim_stories = list(delimited_stories_iterator(test_input))\n",
    "    self.assertEqual('[PROMPT] español [USER] Un niñ [END]', delim_stories[0])\n",
    "    self.assertEqual('\\n[PROMPT] español [USER] Hola [END]', delim_stories[1])\n",
    "    self.assertEqual(test_input, ''.join(delim_stories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3fce254",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilePartitionIterator:\n",
    "    def __init__(self, file_obj, delimiter):\n",
    "        self.delimiter = delimiter\n",
    "        self.file = file_obj\n",
    "        self.buffer = \"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            index = self.buffer.find(self.delimiter)\n",
    "            if index != -1:\n",
    "                partition = self.buffer[:index + len(self.delimiter)]\n",
    "                self.buffer = self.buffer[index + len(self.delimiter):]\n",
    "                return partition\n",
    "\n",
    "            chunk = self.file.read(4096)  # Read in chunks of 4KB\n",
    "            if not chunk:\n",
    "                if self.buffer:\n",
    "                    partition = self.buffer\n",
    "                    self.buffer = \"\"\n",
    "                    return partition\n",
    "                self.file.close()\n",
    "                raise StopIteration\n",
    "\n",
    "            self.buffer += chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3951b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/unittest.status+json": {
       "color": "yellow",
       "message": "",
       "previous": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/unittest.status+json": {
       "color": "lightgreen",
       "message": ".\n----------------------------------------------------------------------\nRan 1 test in 0.000s\n\nOK\n",
       "previous": 0
      },
      "text/plain": [
       "Success"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%unittest_testcase\n",
    "import io\n",
    "def test_file_partition_iterator(self):\n",
    "    test_input = \"\"\"[PROMPT] español [USER] Un niñ [END]\n",
    "[PROMPT] español [USER] Hola [END]\"\"\"\n",
    "    test_input_filelike = io.StringIO(test_input)\n",
    "    delim_stories = list(FilePartitionIterator(test_input_filelike, END_TOKEN))\n",
    "    self.assertEqual('[PROMPT] español [USER] Un niñ [END]', delim_stories[0])\n",
    "    self.assertEqual('\\n[PROMPT] español [USER] Hola [END]', delim_stories[1])\n",
    "    self.assertEqual(test_input, ''.join(delim_stories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74fd684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "END_TOKEN = '[END]'\n",
    "PROMPT_TOKEN = '[PROMPT]'\n",
    "\n",
    "\n",
    "class HackyCheatingLangDetector:\n",
    "    def __init__(self, languages: List[Language]) -> None:\n",
    "        pass\n",
    "\n",
    "\n",
    "    def _detect_single_story(self, story_text) -> List[DetectionResult]:\n",
    "        # story_text = story_text.strip()\n",
    "        # assert story_text.startswith(PROMPT_TOKEN), f\"Unexpected start: {story_text[:20]}\"\n",
    "        task = story_text.split(' ', 2)[1]\n",
    "        assert task in ['english', 'español', 'translate']\n",
    "        paragraph_splitter = re.compile('[^\\n]+')\n",
    "        if task == 'english':\n",
    "            return [DetectionResult(0, len(story_text), 0, Language.ENGLISH)]\n",
    "        elif task == 'español':\n",
    "            return [DetectionResult(0, len(story_text), 0, Language.SPANISH)]\n",
    "        elif task == 'translate':\n",
    "            paragraph_starts = [m.span()[0] for m in re.finditer(paragraph_splitter, story_text)]\n",
    "            ret = []\n",
    "            for paragraph_no, paragraph_start in enumerate(paragraph_starts):\n",
    "                next_paragraph_start = paragraph_starts[paragraph_no + 1] if paragraph_no + 1 < len(paragraph_starts) else len(story_text)+1\n",
    "                lang = Language.ENGLISH if paragraph_no % 2 == 0 else Language.SPANISH\n",
    "                ret.append(DetectionResult(paragraph_start, next_paragraph_start, 0, lang))\n",
    "            return ret\n",
    "\n",
    "  \n",
    "    def split_n_detect(self, text: str, sep: str = \" \") -> List[DetectionResult]:\n",
    "        \"\"\"Split Text and Detect Language.\"\"\"\n",
    "\n",
    "        def merge_results(\n",
    "            results: List[List[DetectionResult]],\n",
    "        ) -> List[DetectionResult]:\n",
    "            \"\"\"Merge Results. If consecutive words are detected as the same language, merge them.\"\"\"\n",
    "            merged_results: list[DetectionResult] = []\n",
    "            for result in results:\n",
    "                if not merged_results:\n",
    "                    merged_results.extend(result)\n",
    "                else:\n",
    "                    for detection in result:\n",
    "                        last_result = merged_results[-1]\n",
    "                        if detection.language == last_result.language:\n",
    "                            merged_results[-1] = DetectionResult(\n",
    "                                language=last_result.language,\n",
    "                                start_index=last_result.start_index,\n",
    "                                end_index=last_result.end_index\n",
    "                                + detection.end_index\n",
    "                                - detection.start_index,\n",
    "                                word_count=last_result.word_count\n",
    "                                + detection.word_count,\n",
    "                            )\n",
    "                        else:\n",
    "                            merged_results.append(\n",
    "                                DetectionResult(\n",
    "                                    language=detection.language,\n",
    "                                    start_index=last_result.end_index,\n",
    "                                    end_index=last_result.end_index\n",
    "                                    + detection.end_index\n",
    "                                    - detection.start_index,\n",
    "                                    word_count=detection.word_count,\n",
    "                                )\n",
    "                            )\n",
    "            return merged_results\n",
    "\n",
    "\n",
    "        results = []\n",
    "        # Detect the language of each part\n",
    "        for story_text in delimited_stories_iterator(text):\n",
    "            results.append(self._detect_single_story(story_text))\n",
    "        \n",
    "        # Merge consecutive results with the same language\n",
    "        merged_results = merge_results(results)\n",
    "        return merged_results\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f79fb5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/unittest.status+json": {
       "color": "yellow",
       "message": "",
       "previous": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/unittest.status+json": {
       "color": "lightgreen",
       "message": ".\n----------------------------------------------------------------------\nRan 1 test in 0.000s\n\nOK\n",
       "previous": 0
      },
      "text/plain": [
       "Success"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%unittest_testcase\n",
    "def test_hacky_cheating_language_detector(self):\n",
    "    test_input = \"\"\"[PROMPT] english [USER] Hi [END]\n",
    "[PROMPT] español [USER] a child [END]\n",
    "[PROMPT] translate [USER] It was night\n",
    "Era de noche\n",
    "Hi\n",
    "Hola [END]\"\"\"\n",
    "    hcld = HackyCheatingLangDetector([])\n",
    "    detections = hcld.split_n_detect(test_input, END_TOKEN)\n",
    "    self.assertEqual(6, len(detections))\n",
    "    expected_detections_with_langs = [\n",
    "        (Language.ENGLISH, '[PROMPT] english [USER] Hi [END]'),\n",
    "        (Language.SPANISH, '\\n[PROMPT] español [USER] a child [END]'),\n",
    "        (Language.ENGLISH, '\\n[PROMPT] translate [USER] It was night'),\n",
    "        (Language.SPANISH, '\\nEra de noche'),\n",
    "        (Language.ENGLISH, '\\nHi'),\n",
    "        (Language.SPANISH, '\\nHola [END]')\n",
    "    ]\n",
    "    for detection, (expected_lang, expected_string) in zip(\n",
    "        detections, expected_detections_with_langs):\n",
    "        self.assertEqual(expected_lang, detection.language)\n",
    "        self.assertEqual(expected_string, test_input[detection.start_index: detection.end_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8aad6bde-9398-430e-a454-294507e0d2a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "special_tokens = ['[PROMPT]', '[USER]', '[END]', '[PAD]', '[BEGIN-EN]', '[BEGIN-ES]']\n",
    "ENGLISH, SPANISH = 0, 1\n",
    "lang_to_token_map = {Language.ENGLISH: '[BEGIN-EN]', Language.SPANISH: '[BEGIN-ES]'}\n",
    "lingua_lang_to_dense_lang_id = {Language.ENGLISH: ENGLISH, Language.SPANISH: SPANISH}\n",
    "supported_languages = [Language.ENGLISH, Language.SPANISH]\n",
    "monolingual_token_file_names = {l: f'{MODEL_PATH}/train_tokenizer_{l}.txt' for l in supported_languages}\n",
    "\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "\n",
    "def detections_to_lang_identified_text(detections: List[DetectionResult], text: str) -> str:\n",
    "    ret = []\n",
    "    for detection in detections:\n",
    "        ret.append(f\"{lang_to_token_map[detection.language]}{text[detection.start_index: detection.end_index]}\")\n",
    "    return ''.join(ret)\n",
    "\n",
    "\n",
    "if DO_TOKENIZATION:\n",
    "    lang_detector = HackyCheatingLangDetector(supported_languages)\n",
    "\n",
    "    os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "    outputs_split_by_lang = {}\n",
    "    for l in supported_languages:        \n",
    "        outputs_split_by_lang[l] = open(monolingual_token_file_names[l], 'w')\n",
    "    segmented_output = open(f'{MODEL_PATH}/train_segmented.txt', 'w')\n",
    "\n",
    "    for single_story in FilePartitionIterator(open(f'{TRAIN_DATA_PATH}/train.txt', 'r'), END_TOKEN):\n",
    "        detections = lang_detector.split_n_detect(single_story, END_TOKEN)              \n",
    "        for detection in detections: \n",
    "            monolingual_fragment = single_story[detection.start_index: detection.end_index]\n",
    "            outputs_split_by_lang[detection.language].write(monolingual_fragment + '\\n')\n",
    "        segmented_output.write(detections_to_lang_identified_text(detections, single_story))  \n",
    "\n",
    "    for l in supported_languages:\n",
    "        outputs_split_by_lang[l].close()\n",
    "    segmented_output.close()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8012fdd8-a814-4f7c-a550-8d0f50870fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers_by_dense_lang_id = []\n",
    "\n",
    "if DO_TOKENIZATION:\n",
    "    for l in supported_languages:\n",
    "        tokenizer = tokenizers.ByteLevelBPETokenizer()\n",
    "        tokenizer.train(\n",
    "            files=[monolingual_token_file_names[l]], \n",
    "            vocab_size=vocab_size, \n",
    "            min_frequency=2,\n",
    "            special_tokens=special_tokens\n",
    "        )\n",
    "        tokenizer.add_special_tokens(special_tokens)\n",
    "        tokenizer.save_model(MODEL_PATH, f'tiny-stories-{l}-bpe')\n",
    "        tokenizers_by_dense_lang_id.append(tokenizer)\n",
    "else:\n",
    "    for l in supported_languages:\n",
    "        tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "            f'{MODEL_PATH}/tiny-stories-{l}-bpe-vocab.json', \n",
    "            f'{MODEL_PATH}/tiny-stories-{l}-bpe-merges.txt'\n",
    "        )\n",
    "        tokenizer.add_special_tokens(special_tokens)\n",
    "        tokenizers_by_dense_lang_id.append(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad25c70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TOK_ID = tokenizers_by_dense_lang_id[0].token_to_id('[PROMPT]')\n",
    "USER_TOK_ID = tokenizers_by_dense_lang_id[0].token_to_id('[USER]')\n",
    "END_TOK_ID = tokenizers_by_dense_lang_id[0].token_to_id('[END]')\n",
    "PAD_TOK_ID = tokenizers_by_dense_lang_id[0].token_to_id('[PAD]')\n",
    "BEGIN_EN_TOK_ID = tokenizers_by_dense_lang_id[0].token_to_id('[BEGIN-EN]')\n",
    "BEGIN_ES_TOK_ID = tokenizers_by_dense_lang_id[0].token_to_id('[BEGIN-ES]')\n",
    "\n",
    "LANG_BEGIN_TOK_IDS = [BEGIN_EN_TOK_ID, BEGIN_ES_TOK_ID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbcb12e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   4,  226,    0, 1457,  226,    1,  509,  362,   17,  264,  418,  512,\n",
       "          432,  264, 1120,   19,  421,  287,  406,  426,  270,  369,  795, 4445,\n",
       "           19,  204,    5,  383,  442,   17,  288,  325,  432,  428,  288,  637,\n",
       "           19,  470,  355,  416,  311,  410,  874, 1838,   19,  204,    4,  304,\n",
       "         2794,  412,  710,  948,  972,  226,    2], device='cuda:0',\n",
       "        dtype=torch.int16),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0], device='cuda:0', dtype=torch.int16))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_splitter_re = re.compile(r'\\[BEGIN-(EN|ES)\\]')\n",
    "lang_code_to_lang_dense_id = {'EN': ENGLISH, 'ES': SPANISH}\n",
    "\n",
    "def multi_tokenize_delimited_story(text):\n",
    "    assert text.startswith('[BEGIN-EN]') or text.startswith('[BEGIN-ES]')\n",
    "    # assert text.endswith(END_TOKEN)\n",
    "\n",
    "    # This splits text into sequences of [lang_code_1, text_1, lang_code_2, text_2, ...]\n",
    "    # where lang_code_i is either 'EN' or 'ES'\n",
    "    per_lang_splits = [p for p in lang_splitter_re.split(text) if p.strip()]\n",
    "    assert len(per_lang_splits) % 2 == 0\n",
    "    lang_dense_id_per_text = [lang_code_to_lang_dense_id[code] for code in per_lang_splits[::2]]\n",
    "    monolingual_texts = per_lang_splits[1::2]\n",
    "\n",
    "    token_ids, lang_ids = [], []\n",
    "    for lang_dense_id, monolingual_text in zip(lang_dense_id_per_text, monolingual_texts):\n",
    "        tokenizer = tokenizers_by_dense_lang_id[lang_dense_id]\n",
    "        \n",
    "        these_tokens = [LANG_BEGIN_TOK_IDS[lang_dense_id]] + tokenizer.encode(monolingual_text).ids \n",
    "        token_ids.extend(these_tokens)\n",
    "        lang_ids.extend([lang_dense_id] * len(these_tokens))\n",
    "    \n",
    "    return torch.tensor(token_ids, dtype=torch.short), torch.tensor(lang_ids, dtype=torch.short)\n",
    "test_input = (\n",
    "    \"\"\"[BEGIN-EN] [PROMPT] translate [USER] One day, a little boy saw a cake. It was very big and had many strawberries.\n",
    "[BEGIN-ES] Un día, un niño pequeño vio un pastel. Era muy grande y tenía muchas fresas.\n",
    "[BEGIN-EN] Slightly more text [END]\"\"\")\n",
    "multi_tokenize_delimited_story(test_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "601cc17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEGIN-EN] [PROMPT] translate [USER] One day, a little boy saw a cake. It was very big and had many strawberries.\n",
      "[BEGIN-ES] Un día, un niño pequeño vio un pastel. Era muy grande y tenía muchas fresas.\n",
      "[BEGIN-EN] Slightly more text [END]\n"
     ]
    }
   ],
   "source": [
    "def multi_detokenize(tokens, langs):\n",
    "    ret = \"\"\n",
    "    for token, lang in zip(tokens, langs):\n",
    "        ret += tokenizers_by_dense_lang_id[lang].decode([token], skip_special_tokens=False)\n",
    "    return ret\n",
    "\n",
    "print(multi_detokenize(*multi_tokenize_delimited_story(test_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99f28087",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_TOKENIZATION:\n",
    "    output_buf = []\n",
    "    num_outputs = 0\n",
    "    for story in tqdm(FilePartitionIterator(open(f'{MODEL_PATH}/train_segmented.txt', 'r'), END_TOKEN)):\n",
    "        output_buf.append(multi_tokenize_delimited_story(story))\n",
    "\n",
    "        if len(output_buf) > 500_000:\n",
    "            torch.save(output_buf, f'{MODEL_PATH}/tokenized-{num_outputs}.pt')\n",
    "            num_outputs += 1\n",
    "            if num_outputs == 1:\n",
    "                torch.save(output_buf[:1000], f'{MODEL_PATH}/tiny-shard.pt')\n",
    "            output_buf = []\n",
    "           \n",
    "\n",
    "    if output_buf:\n",
    "        torch.save(output_buf, f'{MODEL_PATH}/tokenized-{num_outputs}.pt')\n",
    "        num_outputs += 1\n",
    "        output_buf = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d27547a-ec20-40db-ab71-7dfd168330b7",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f43fb8b-987f-4d14-9712-ba2758335308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [06:33<00:00, 393.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in stories:  500001\n",
      "length of stories in tokens 103573212\n",
      "# stories longer than 256 : 74177 out of 1000000, 7.42%\n"
     ]
    }
   ],
   "source": [
    "def load_sharded_story(shard_no):\n",
    "    return torch.load(f'{MODEL_PATH}/tokenized-{shard_no}.pt')\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    # load the tokenized stories in parallel using threads\n",
    "    # this is faster than loading them sequentially\n",
    "\n",
    "    if not TINY_DATA_LOAD:\n",
    "        num_shards = len(glob.glob(f'{MODEL_PATH}/tokenized-*'))\n",
    "        from concurrent.futures import ThreadPoolExecutor\n",
    "        with ThreadPoolExecutor() as pool:\n",
    "            stories = list(tqdm(pool.map(load_sharded_story, range(num_shards)), total=num_shards))\n",
    "    else:\n",
    "        stories = [torch.load(f'{MODEL_PATH}/tiny-shard.pt')]\n",
    "\n",
    "    all_stories = []\n",
    "    for story in stories:\n",
    "        all_stories.extend(story)\n",
    "    random.seed(1337)\n",
    "    random.shuffle(all_stories)\n",
    "\n",
    "    print(\"length of dataset in stories: \", len(all_stories))\n",
    "    print(\"length of stories in tokens\", sum(len(story[0]) for story in all_stories))\n",
    "\n",
    "    num_stories_to_check = 1_000_000\n",
    "    num_long = sum(len(story[0]) > T for story in all_stories[:num_stories_to_check])\n",
    "    print(\n",
    "        f\"# stories longer than {T} : {num_long} out of {num_stories_to_check}, {num_long/num_stories_to_check:.2%}\")\n",
    "\n",
    "    n = int(0.9*len(all_stories))\n",
    "\n",
    "    train_data = all_stories[:n]  # use prepared splits instead, \n",
    "    val_data = all_stories[n:]  # segregate validation data by task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b29c76d-2e50-47a0-bb79-3a8d7819071d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  4,   0, 400, 226,   1], device='cuda:0')\n",
      "tensor([  0, 400, 226,   1, 491], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    # remove y_land_idxs from return value\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # ix = torch.randint(0, len(data), (B,)) \n",
    "    ix = [i for i in range(B)]  # HACK! for now, just use the first B stories\n",
    "    \n",
    "    x = torch.full((B, T), PAD_TOK_ID, dtype=torch.long)\n",
    "    y = torch.full((B, T), PAD_TOK_ID, dtype=torch.long)\n",
    "    x_lang_idxs = torch.full((B, T), 0, dtype=torch.long)\n",
    "    y_lang_idxs = torch.full((B, T), 0, dtype=torch.long)\n",
    "\n",
    "    for sequence_index, random_story_index in enumerate(ix):\n",
    "        story_tokens = data[random_story_index][0].long()[:T - 1]\n",
    "        story_lang_idxs = data[random_story_index][1].long()[:T - 1]\n",
    "\n",
    "        story_length = story_tokens.shape[0]\n",
    "        assert story_lang_idxs.shape == story_lang_idxs.shape\n",
    "        \n",
    "        x[sequence_index][0:story_length-1] = story_tokens[0:story_length-1]\n",
    "        y[sequence_index][0:story_length-1] = story_tokens[1:story_length]\n",
    "        x_lang_idxs[sequence_index][0: story_length-1] = story_lang_idxs[0:story_length-1]\n",
    "        y_lang_idxs[sequence_index][0: story_length-1] = story_lang_idxs[1:story_length]\n",
    "\n",
    "    return x, y, x_lang_idxs, y_lang_idxs\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    xb, yb, x_lang_idxs, y_lang_idxs = get_batch('train')\n",
    "\n",
    "\n",
    "    print(xb[0][:5])\n",
    "    print(yb[0][:5])\n",
    "    print(x_lang_idxs[0][:5])\n",
    "    print(y_lang_idxs[0][:5])\n",
    "    language = x_lang_idxs[0,0]\n",
    "    print(language)\n",
    "    # print(decode(xb[0].tolist(), language.item())) # the zero on xb is the first story. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaadce6",
   "metadata": {},
   "source": [
    "## Model defintions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a3ed8ea-281b-45fa-928a-b856abc13d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    '''One Head of self-attention'''\n",
    "    def __init__(self, H):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(C, H, bias=False)\n",
    "        self.key = nn.Linear(C, H, bias=False)\n",
    "        self.value = nn.Linear(C, H, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(T, T)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        query_vectors = self.query(x)\n",
    "        key_vectors = self.key(x)\n",
    "\n",
    "\n",
    "        # Attention masking(so we can't look into the past):\n",
    "        tril = self.tril\n",
    "        wei = torch.zeros(T, T) \n",
    "        wei = wei.masked_fill(tril == 0, float('-inf')) # set the upper triangular to -inf\n",
    "\n",
    "        # multiply the two to get the attention weights\n",
    "        attention_pattern = query_vectors @ key_vectors.transpose(-2, -1) # T, T\n",
    "        attention_pattern = attention_pattern / (H ** 0.5) # scale the attention pattern for numerical stability\n",
    "        attention_weights = F.softmax(attention_pattern + wei, dim=-1) # T, T (the row dimension is the query)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        value_vectors = self.value(x) # the direction we should go in the embedding space for each token (ie more blue) T, H\n",
    "\n",
    "        # apply the attention weights to the value vectors\n",
    "        context = attention_weights @ value_vectors # T, H\n",
    "\n",
    "        # project back into original space from value space\n",
    "        return context\n",
    "\n",
    "x = torch.randn(B,T,C)\n",
    "head = Head(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a1ddfac-31a8-43b6-ac7f-646dbea39fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''Multiple heads of self-attention'''\n",
    "    def __init__(self, H, C, n_heads): # H is head embedding space size, n_heads is number of heads\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(H) for _ in range(n_heads)])\n",
    "        self.combine_heads = nn.Linear(H*n_heads, C)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        x = self.combine_heads(x)  # T, C\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e64c34e5-ad29-440c-ab9c-0b8c4c153087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 32])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head = MultiHeadAttention(H, C, n_heads)\n",
    "head.heads[0].forward(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0beeabe-00ec-45c8-9911-366fec57a5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    '''Feed-forward neural network'''\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(C, C * feedforward_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(C * feedforward_factor, C),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "450533d3-26cb-49f0-afde-777f1b6d8e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    '''Layer normalization'''\n",
    "    def __init__(self, C, use_affine=True):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(C)) if use_affine else None\n",
    "        self.beta = nn.Parameter(torch.zeros(C)) if use_affine else None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        if self.gamma is not None and self.beta is not None:\n",
    "            return self.gamma * (x - mean) / (std + 1e-6) + self.beta\n",
    "        else:\n",
    "            return (x - mean) / (std + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75b40957-1158-4e85-9b07-6a25d72338e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''Transformer block'''\n",
    "    def __init__(self, H, C, n_heads):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(H, C, n_heads)\n",
    "        self.ff = FeedForward(C)\n",
    "        self.norm1 = LayerNorm(C, use_affine=True)\n",
    "        self.norm2 = LayerNorm(C, use_affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6224a0ef-2693-4e29-b989-3e80a046bd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters in the model (millions):  12.825856\n"
     ]
    }
   ],
   "source": [
    "class MultiTokenizerGPT(nn.Module):\n",
    "    def __init__(self, n_layers, n_languages):\n",
    "        super().__init__()\n",
    "        self.token_embedding_tables = [nn.Embedding(vocab_size, C) for _ in range(n_languages)]\n",
    "        self.position_embedding_table = nn.Embedding(T, C)\n",
    "        self.lm_heads = nn.ModuleList([nn.Linear(C, vocab_size) for _ in range(n_languages)])\n",
    "        self.layers = nn.ModuleList([Block(H, C, n_heads) for _ in range(n_layers)])\n",
    "        self.block = nn.ModuleList([Block(H, C, n_heads)])\n",
    "    \n",
    "    # token_lang_idx has same shape as token_ids, encodes langs\n",
    "    def forward(self, token_ids, token_lang_idxs, targets=None):\n",
    "        B, T = token_ids.shape\n",
    "\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T))\n",
    "\n",
    "        token_emb = torch.zeros(B*T, C)\n",
    "\n",
    "        token_ids_flat = token_ids.view(-1)\n",
    "\n",
    "        unique_lang_idxs = torch.unique(token_lang_idxs)\n",
    "\n",
    "        # List of tensors, where each tensor contains the indexes in the flat tokens list of all tokens of a language.\n",
    "        # there are as many tensors as there are languages present in the batch. Total length of all tensors = # tokens\n",
    "        lang_tok_mapping = [torch.where(token_lang_idxs.view(-1) == idx) for idx in unique_lang_idxs]\n",
    "\n",
    "        for lang_id, monolingual_tokens_idx in zip(unique_lang_idxs, lang_tok_mapping):\n",
    "            lang_embedding_table = self.token_embedding_tables[lang_id]\n",
    "            monolingual_tok_emb = lang_embedding_table(token_ids_flat[monolingual_tokens_idx])\n",
    "            token_emb[monolingual_tokens_idx] = monolingual_tok_emb\n",
    "\n",
    "        x = token_emb.view(B, T, C) + pos_emb # token identities and positions contained\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x_flat = x.view(B*T, C)\n",
    "        logits = torch.zeros(B*T, vocab_size)\n",
    "\n",
    "        for lang_id, monolingual_tokens_idx in zip(unique_lang_idxs, lang_tok_mapping):\n",
    "            lm_head = self.lm_heads[lang_id]\n",
    "            new_logits = lm_head(x_flat[monolingual_tokens_idx])\n",
    "            logits[monolingual_tokens_idx] = new_logits\n",
    "\n",
    "        logits = logits.view(B,T,vocab_size)\n",
    "\n",
    "        batch_dim, sequence_dim, embedding_dim = logits.size()\n",
    "\n",
    "        if targets is None:\n",
    "            return logits, None\n",
    "        else:\n",
    "            logits_loss_view = logits.view(-1, vocab_size) \n",
    "            targets_loss_view = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits_loss_view, targets_loss_view)\n",
    "            return logits, loss\n",
    "\n",
    "        return idx\n",
    "    def prompt_model(self, prompt, max_new_tokens, language=\"spanish\", temperature=0.5):\n",
    "        token_lang_idxs = torch.full([1, T], language_id)\n",
    "\n",
    "        autoregressive_seq = [START_TOK, language_token]\n",
    "\n",
    "        autoregressive_seq += encode(prompt, language_id)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            prediction_index = len(autoregressive_seq)-1\n",
    "\n",
    "            model_input = torch.tensor(autoregressive_seq)\n",
    "            \n",
    "            while model_input.shape[0] < T:\n",
    "                pad_token = torch.tensor([PAD_TOK], dtype=torch.long)\n",
    "                model_input = torch.cat((model_input, pad_token), dim=0)\n",
    "\n",
    "            model_input = model_input.unsqueeze(0)\n",
    "\n",
    "            logits, loss = model.forward(\n",
    "                token_ids=model_input,\n",
    "                token_lang_idxs=token_lang_idxs\n",
    "                )\n",
    "            prediction_token = logits[:, prediction_index, :] / temperature\n",
    "            probabilities = F.softmax(prediction_token, dim=-1)\n",
    "            next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "            next_token = next_token.item()\n",
    "            if(next_token == END_TOK):\n",
    "                break\n",
    "\n",
    "            autoregressive_seq.append(next_token)\n",
    "        # get the autoregressive sequence\n",
    "        return decode(autoregressive_seq, language_id)\n",
    "\n",
    "\n",
    "model = MultiTokenizerGPT(n_layers, 2)\n",
    "# get the number of parameters in the model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"number of parameters in the model (millions): \", count_parameters(model) /1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34e95468-ce3b-4351-9355-2859a0d52979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256, 8192])\n",
      "tensor(9.8065, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_MODEL:\n",
    "    xb, yb, x_lang_idxs, y_lang_idxs = get_batch('train')\n",
    "\n",
    "    logits, loss = model(\n",
    "        token_ids = xb,\n",
    "        token_lang_idxs = x_lang_idxs,\n",
    "        targets = yb\n",
    "    )\n",
    "    print(logits.shape)\n",
    "    print(loss)\n",
    "\n",
    "    # test_idx = torch.zeros(1, T).long()\n",
    "    # model.forward(idx=test_idx)\n",
    "    # decode(model.generate(idx=test_idx, max_new_tokens=100)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20d745fa-d48c-441c-886e-890c2a7636aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae2684ea-3417-4708-b49d-03acf0e1dc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 10\n",
    "eval_interval = 300\n",
    "chars_per_token=3.9   # HACK!  need to compute chars per token for fair comparision\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(is_last=False):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        real_iters = eval_iters\n",
    "        if is_last and split == 'val':  # increase last eval to mitigate noise\n",
    "            real_iters *= 10 \n",
    "        losses = torch.zeros(real_iters)\n",
    "        for k in range(real_iters):\n",
    "            # X, Y = get_batch(split)\n",
    "            xb, yb, x_lang_idxs, y_lang_idxs = get_batch(split)\n",
    "\n",
    "            logits, loss = model(\n",
    "                token_ids = xb,\n",
    "                token_lang_idxs = x_lang_idxs,\n",
    "                targets = yb\n",
    "            )\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean() / chars_per_token\n",
    "    model.train()\n",
    "    return out\n",
    "    \n",
    "if TRAIN_MODEL:\n",
    "    estimate_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2b37613-849e-46a5-9e67-64c432bd9ebd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/60000 [00:02<45:16:45,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 2.1928205490112305, 'val': 2.1927239894866943}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 301/60000 [02:04<18:28:04,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.003662200877442956, 'val': 2.196388006210327}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 601/60000 [04:07<18:38:10,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.003520442871376872, 'val': 2.3756940364837646}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 901/60000 [06:13<18:31:48,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.0035052387975156307, 'val': 2.478118419647217}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1201/60000 [08:19<18:26:07,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.003485034452751279, 'val': 2.551088571548462}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1501/60000 [10:25<18:19:46,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.003485502442345023, 'val': 2.605969190597534}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1801/60000 [12:30<18:15:02,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.0034806441981345415, 'val': 2.6513237953186035}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 2101/60000 [14:36<18:09:29,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.0034874631091952324, 'val': 2.6841843128204346}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2401/60000 [16:42<18:04:02,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.0034836484119296074, 'val': 2.718130588531494}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 2701/60000 [18:47<18:00:38,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.0034806702751666307, 'val': 2.743708848953247}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 3000/60000 [20:50<6:29:11,  2.44it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.0034769168123602867, 'val': 2.7661924362182617}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3301/60000 [22:58<17:43:52,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.0034794809762388468, 'val': 2.7859864234924316}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3601/60000 [25:04<17:38:37,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.003487480105832219, 'val': 2.8029568195343018}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 3901/60000 [27:09<17:34:41,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.0034870679955929518, 'val': 2.821474075317383}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 4201/60000 [29:16<17:33:46,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.0034773831721395254, 'val': 2.834867238998413}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4501/60000 [31:22<17:24:16,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.003483883338049054, 'val': 2.8516030311584473}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4801/60000 [33:28<17:20:37,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.003478092374280095, 'val': 2.865288496017456}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 5101/60000 [35:35<17:15:37,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.003477746155112982, 'val': 2.8798277378082275}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 5401/60000 [37:41<17:06:44,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.003478235099464655, 'val': 2.892437219619751}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 5701/60000 [39:47<17:05:58,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.003475240198895335, 'val': 2.911022424697876}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 6000/60000 [41:50<6:10:39,  2.43it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.0034786956384778023, 'val': 2.925156354904175}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 6301/60000 [43:59<16:51:02,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.003479156643152237, 'val': 2.945807695388794}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 6601/60000 [46:05<16:45:09,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.0034747091121971607, 'val': 2.9658756256103516}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6901/60000 [48:11<16:41:44,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.003480019513517618, 'val': 2.986574649810791}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 7201/60000 [50:17<16:33:44,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.004078482743352652, 'val': 2.3146564960479736}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 7501/60000 [52:22<16:25:51,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.003615772118791938, 'val': 2.583364963531494}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 7801/60000 [54:27<16:19:30,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.0035413065925240517, 'val': 2.7271509170532227}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 8101/60000 [56:33<16:16:08,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.0035132381599396467, 'val': 2.831728935241699}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 8401/60000 [58:38<16:13:16,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.0034991675056517124, 'val': 2.9166946411132812}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 8646/60000 [1:00:19<5:58:19,  2.39it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m xb, yb, x_lang_idxs, y_lang_idxs \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# loss\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_lang_idxs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_lang_idxs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[25], line 40\u001b[0m, in \u001b[0;36mMultiTokenizerGPT.forward\u001b[0;34m(self, token_ids, token_lang_idxs, targets)\u001b[0m\n\u001b[1;32m     37\u001b[0m logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(B\u001b[38;5;241m*\u001b[39mT, vocab_size)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lang_id, monolingual_tokens_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(unique_lang_idxs, lang_tok_mapping):\n\u001b[0;32m---> 40\u001b[0m     lm_head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_heads\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlang_id\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     41\u001b[0m     new_logits \u001b[38;5;241m=\u001b[39m lm_head(x_flat[monolingual_tokens_idx])\n\u001b[1;32m     42\u001b[0m     logits[monolingual_tokens_idx] \u001b[38;5;241m=\u001b[39m new_logits\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/container.py:295\u001b[0m, in \u001b[0;36mModuleList.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mvalues())[idx])\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_abs_string_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/container.py:283\u001b[0m, in \u001b[0;36mModuleList._get_abs_string_index\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_abs_string_index\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m    282\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the absolute index for the list of modules.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m     idx \u001b[38;5;241m=\u001b[39m \u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)):\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is out of range\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if TRAIN_MODEL:\n",
    "    dump_model_interval = 1000\n",
    "\n",
    "    for steps in tqdm(range(0, max_iters)):\n",
    "        xb, yb, x_lang_idxs, y_lang_idxs = get_batch('train')\n",
    "        # loss\n",
    "        logits, loss = model(\n",
    "            token_ids = xb,\n",
    "            token_lang_idxs = x_lang_idxs,\n",
    "            targets = yb\n",
    "        )\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if steps % eval_interval == 0:\n",
    "            losses = estimate_loss()\n",
    "            # wandb.log({\"train\": losses['train'].item(), \"val\": losses['val'].item(), \"l2\":l2})\n",
    "            print({\"train\": losses['train'].item(), \"val\": losses['val'].item()})\n",
    "        if steps % dump_model_interval == 0 and steps > 0:\n",
    "            model_no = steps // dump_model_interval\n",
    "            torch.save(model.state_dict(), f'{MODEL_PATH}/tiny-stories-model-{model_no}.pt')\n",
    "\n",
    "    losses = estimate_loss(is_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f5ea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), f'{MODEL_PATH}/overfit-batch-1-model.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62703f61",
   "metadata": {},
   "source": [
    "## Manual prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5637b3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAIN_MODEL:\n",
    "    # model.load_state_dict(torch.load(f'{MODEL_PATH}/tiny-stories-model-18.pt',  map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b05e4a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 38\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m autoregressive_toks, autoregressive_langs\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# get the autoregressive sequence\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# return decode(autoregressive_seq, language_id)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# generate prompt -> hacky split -> multi encod\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m prompted \u001b[38;5;241m=\u001b[39m (\u001b[43mdata_split\u001b[49m\u001b[38;5;241m.\u001b[39mwrite_translation_story_tinyprompt_strs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLily fue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLily went to the park.\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     39\u001b[0m lang_segmented \u001b[38;5;241m=\u001b[39m detections_to_lang_identified_text(HackyCheatingLangDetector([])\u001b[38;5;241m.\u001b[39msplit_n_detect(prompted, END_TOKEN), prompted)\n\u001b[1;32m     40\u001b[0m lang_segmented \u001b[38;5;241m=\u001b[39m lang_segmented\u001b[38;5;241m.\u001b[39mremovesuffix(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[END]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_split' is not defined"
     ]
    }
   ],
   "source": [
    "def prompt_model(model, tokens, lang_idxs, max_new_tokens, temperature=0.5):\n",
    "    autoregressive_toks = tokens.tolist()\n",
    "    autoregressive_langs = lang_idxs.tolist()\n",
    "\n",
    "    assert len(autoregressive_toks) == len(autoregressive_langs)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        prediction_index = len(autoregressive_toks)-1\n",
    "\n",
    "        input_tokens = torch.tensor(autoregressive_toks)\n",
    "        input_langs = torch.tensor(autoregressive_langs)\n",
    "        while input_tokens.shape[0] < T:\n",
    "            input_tokens = torch.cat((input_tokens, torch.tensor([PAD_TOK_ID])))\n",
    "            input_langs = torch.cat((input_langs, torch.tensor([autoregressive_langs[-1]])))\n",
    "\n",
    "        input_tokens = input_tokens.unsqueeze(0)\n",
    "        input_langs = input_langs.unsqueeze(0)\n",
    "\n",
    "        logits, ignored_loss = model.forward(token_ids=input_tokens, token_lang_idxs=input_langs)\n",
    "        temp_scaled_logits = logits[:, prediction_index, :] / temperature\n",
    "        probabilities = F.softmax(temp_scaled_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probabilities, num_samples=1).item()\n",
    "\n",
    "        if next_token in LANG_BEGIN_TOK_IDS:\n",
    "            print('switching langs')\n",
    "            autoregressive_langs.append(LANG_BEGIN_TOK_IDS.index(next_token))\n",
    "        else:\n",
    "            autoregressive_langs.append(autoregressive_langs[-1])\n",
    "\n",
    "        autoregressive_toks.append(next_token)\n",
    "        if next_token == END_TOK_ID:\n",
    "            break\n",
    "    return autoregressive_toks, autoregressive_langs\n",
    "    # get the autoregressive sequence\n",
    "    # return decode(autoregressive_seq, language_id)\n",
    "\n",
    "# generate prompt -> hacky split -> multi encod\n",
    "prompted = (data_split.write_translation_story_tinyprompt_strs('Lily fue', 'Lily went to the park.'))\n",
    "lang_segmented = detections_to_lang_identified_text(HackyCheatingLangDetector([]).split_n_detect(prompted, END_TOKEN), prompted)\n",
    "lang_segmented = lang_segmented.removesuffix('[END]')\n",
    "\n",
    "x, lang_x = multi_tokenize_delimited_story(lang_segmented)\n",
    "print(multi_detokenize(x, lang_x))\n",
    "comp_tokens, comp_langs = prompt_model(model, x, lang_x, 10, temperature=.01)\n",
    "multi_detokenize(comp_tokens, comp_langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a71a0ceb-ce02-4910-b814-68b54c0927c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "xb, xl, yb, yl = get_batch('train')\n",
    "print(xb.shape[0])\n",
    "print(xl.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e775b75",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:3\u001b[0;36m\u001b[0m\n\u001b[0;31m    max_new_tokens=100,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    ")model.prompt_model(\n",
    "        prompt=\"Diego tenia un perro grande.\",\n",
    "    max_new_tokens=100,\n",
    "    language=\"english\",\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf25a21",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
