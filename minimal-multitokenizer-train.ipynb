{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da08a5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_TOKENIZATION=True\n",
    "TRAIN_MODEL=False\n",
    "# DATA_PREFIX='expt_1'\n",
    "DATA_PREFIX='tiny_sample'\n",
    "MODEL_EXPT_NAME=\"multitok_model_1\"\n",
    "\n",
    "TRAIN_DATA_PATH=f'train-model/{DATA_PREFIX}'\n",
    "MODEL_PATH=f'train-model/{DATA_PREFIX}/{MODEL_EXPT_NAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7726f1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lingua in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.15.0)\n",
      "Requirement already satisfied: polib>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lingua) (1.2.0)\n",
      "Requirement already satisfied: click>=8.0.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lingua) (8.1.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: multi-tokenizer in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.1.4)\n",
      "Requirement already satisfied: lingua-language-detector<3.0.0,>=2.0.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from multi-tokenizer) (2.0.2)\n",
      "Requirement already satisfied: tokenizers<0.20.0,>=0.19.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from multi-tokenizer) (0.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (0.23.4)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (2024.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (6.0.1)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (2024.6.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting ipython-unittest\n",
      "  Downloading ipython_unittest-0.3.2-py2.py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: ipython in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython-unittest) (8.17.2)\n",
      "Requirement already satisfied: decorator in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ipython-unittest) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ipython-unittest) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ipython-unittest) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ipython-unittest) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ipython-unittest) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ipython-unittest) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ipython-unittest) (5.14.3)\n",
      "Requirement already satisfied: exceptiongroup in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ipython-unittest) (1.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ipython-unittest) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jedi>=0.16->ipython->ipython-unittest) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pexpect>4.3->ipython->ipython-unittest) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython->ipython-unittest) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from stack-data->ipython->ipython-unittest) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from stack-data->ipython->ipython-unittest) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from stack-data->ipython->ipython-unittest) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython->ipython-unittest) (1.16.0)\n",
      "Downloading ipython_unittest-0.3.2-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: ipython-unittest\n",
      "Successfully installed ipython-unittest-0.3.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install lingua \n",
    "!pip install multi-tokenizer\n",
    "!pip install ipython-unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "feadb260-56af-4aa5-951a-55d3a04d157e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from itertools import chain\n",
    "import tokenizers\n",
    "    \n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from itertools import islice\n",
    "from typing import List\n",
    "\n",
    "%load_ext ipython_unittest\n",
    "\n",
    "from multi_tokenizer import LanguageDetector\n",
    "from lingua import DetectionResult, Language as LinguaLanguage \n",
    "\n",
    "\n",
    "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_default_device(device)\n",
    "# assert device == 'cuda', \"This notebook is not optimized for CPU\"\n",
    "if device == 'cpu' and TRAIN_MODEL:\n",
    "    print(\"Warning: Training on CPU will be slow\")\n",
    "    assert False\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"eval_interval\": 300,\n",
    "    \"max_iters\": 60000, \n",
    "    \"H\": 32, # per head dimension size\n",
    "    \"B\": 64, # batch size\n",
    "    \"T\": 256, # Sequence length\n",
    "    \"C\": 256, # model size\n",
    "    \"feedforward_factor\": 3,\n",
    "    \"n_heads\": 8,\n",
    "    \"dropout\": 0.0,\n",
    "    \"l2_penalty\": 0.0,\n",
    "    \"n_layers\": 12,\n",
    "    \"vocab_size\": 2**13,\n",
    "    # \"git_hash\": os.popen(\"git rev-parse HEAD\").read().strip()\n",
    "}\n",
    "\n",
    "for k,v in config.items():\n",
    "    locals()[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1585aecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/unittest.status+json": {
       "color": "yellow",
       "message": "",
       "previous": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/unittest.status+json": {
       "color": "lightgreen",
       "message": ".\n----------------------------------------------------------------------\nRan 1 test in 0.000s\n\nOK\n",
       "previous": 0
      },
      "text/plain": [
       "Success"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%unittest_testcase \n",
    "def batch_iterator(iterable, batch_size):\n",
    "    iterator = iter(iterable)\n",
    "    return iter(lambda: list(islice(iterator, batch_size)), [])\n",
    "\n",
    "def test_sample_batch_iterator(self):\n",
    "    data = list(range(5))\n",
    "    l = batch_iterator(data, 2)\n",
    "    self.assertEqual(next(l), [0, 1])\n",
    "    self.assertEqual(next(l), [2, 3])\n",
    "    self.assertEqual(next(l), [4])\n",
    "    with self.assertRaises(StopIteration):\n",
    "        next(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fd684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For The input train txt file, below will be the output format given by running this hacky lang detect code:\n",
    "\n",
    "Input text file:\n",
    "\n",
    "[PROMPT] español [USER] Un niñ [END]\n",
    "[PROMPT] español [USER] Hola [END]\n",
    "[PROMPT] english [USER] Hi [END]\n",
    "[PROMPT] español [USER] a child [END]\n",
    "[PROMPT] translate [USER] It was night\n",
    "Era de noche \n",
    "Hi\n",
    "Hola [END]\n",
    "[PROMPT] english [USER] It was a night [END]\n",
    "\n",
    "\n",
    "\n",
    "Output text file with appended lang ids:\n",
    "\n",
    "[BEGIN-ES] [PROMPT] español [USER] Un niñ [END]\n",
    "[PROMPT] español [USER] Hola [END]\n",
    "[BEGIN-EN] [PROMPT] english [USER] Hi [END]\n",
    "[BEGIN-ES] [PROMPT] español [USER] a child [END]\n",
    "[BEGIN-EN] [PROMPT] translate [USER] It was night\n",
    "[BEGIN-ES] Era de noche\n",
    "[BEGIN-EN] Hi\n",
    "[BEGIN-ES] Hola [END]\n",
    "[BEGIN-EN] [PROMPT] english [USER] It was a night [END]\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "END_TOKEN = '[END]'\n",
    "PROMPT_TOKEN = '[PROMPT]'\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from lingua import DetectionResult, Language, LanguageDetectorBuilder\n",
    "\n",
    "\n",
    "class HackyCheatingLangDetector:\n",
    "    def __init__(self, languages: List[Language]) -> None:\n",
    "        \"\"\"Initialize Language Detector.\"\"\"\n",
    "        self.languages = languages\n",
    "        self.detector = LanguageDetectorBuilder.from_languages(*languages).build()\n",
    "\n",
    "    def _delimited_stories_iterator(self, text):\n",
    "        start = 0\n",
    "        last_match_end = text.find(END_TOKEN)\n",
    "        while last_match_end != -1:\n",
    "            yield text[start:last_match_end + len(END_TOKEN)]\n",
    "            start = last_match_end + len(END_TOKEN) #+ 1 # Add one to skip the space\n",
    "            last_match_end = text.find(END_TOKEN, start)\n",
    "\n",
    "    def _detect_single_story(self, story_text) -> List[DetectionResult]:\n",
    "        # story_text = story_text.strip()\n",
    "        # assert story_text.startswith(PROMPT_TOKEN), f\"Unexpected start: {story_text[:20]}\"\n",
    "        task = story_text.split(' ', 2)[1]\n",
    "        assert task in ['english', 'español', 'translate']\n",
    "        paragraph_splitter = re.compile('[^\\n]+')\n",
    "        if task == 'english':\n",
    "            return [DetectionResult(0, len(story_text), 0, LinguaLanguage.ENGLISH)]\n",
    "        elif task == 'español':\n",
    "            return [DetectionResult(0, len(story_text), 0, LinguaLanguage.SPANISH)]\n",
    "        elif task == 'translate':\n",
    "            paragraph_starts = [m.span()[0] for m in re.finditer(paragraph_splitter, story_text)]\n",
    "            ret = []\n",
    "            for paragraph_no, paragraph_start in enumerate(paragraph_starts):\n",
    "                next_paragraph_start = paragraph_starts[paragraph_no + 1] if paragraph_no + 1 < len(paragraph_starts) else len(story_text)+1\n",
    "                lang = LinguaLanguage.ENGLISH if paragraph_no % 2 == 0 else LinguaLanguage.SPANISH\n",
    "                ret.append(DetectionResult(paragraph_start, next_paragraph_start, 0, lang))\n",
    "            return ret\n",
    "\n",
    "  \n",
    "    def split_n_detect(self, text: str, sep: str = \" \") -> List[DetectionResult]:\n",
    "        \"\"\"Split Text and Detect Language.\"\"\"\n",
    "\n",
    "        def merge_results(\n",
    "            results: List[List[DetectionResult]],\n",
    "        ) -> List[DetectionResult]:\n",
    "            \"\"\"Merge Results. If consecutive words are detected as the same language, merge them.\"\"\"\n",
    "            merged_results: list[DetectionResult] = []\n",
    "            for result in results:\n",
    "                if not merged_results:\n",
    "                    merged_results.extend(result)\n",
    "                else:\n",
    "                    for detection in result:\n",
    "                        last_result = merged_results[-1]\n",
    "                        if detection.language == last_result.language:\n",
    "                            merged_results[-1] = DetectionResult(\n",
    "                                language=last_result.language,\n",
    "                                start_index=last_result.start_index,\n",
    "                                end_index=last_result.end_index\n",
    "                                + detection.end_index\n",
    "                                - detection.start_index,\n",
    "                                word_count=last_result.word_count\n",
    "                                + detection.word_count,\n",
    "                            )\n",
    "                        else:\n",
    "                            merged_results.append(\n",
    "                                DetectionResult(\n",
    "                                    language=detection.language,\n",
    "                                    start_index=last_result.end_index,\n",
    "                                    end_index=last_result.end_index\n",
    "                                    + detection.end_index\n",
    "                                    - detection.start_index,\n",
    "                                    word_count=detection.word_count,\n",
    "                                )\n",
    "                            )\n",
    "            return merged_results\n",
    "\n",
    "\n",
    "        results = []\n",
    "        # Detect the language of each part\n",
    "        for story_text in self._delimited_stories_iterator(text):\n",
    "            results.append(self._detect_single_story(story_text))\n",
    "        \n",
    "        # Merge consecutive results with the same language\n",
    "        merged_results = merge_results(results)\n",
    "        return merged_results\n",
    "\n",
    "\n",
    "supported_languages = [LinguaLanguage.ENGLISH, LinguaLanguage.SPANISH]\n",
    "lang_detector = HackyCheatingLangDetector(supported_languages)\n",
    "monolingual_token_file_names = {l: f'{MODEL_PATH}/train_tokenizer_{l}.txt' for l in supported_languages}\n",
    "lang_to_token_map = {LinguaLanguage.ENGLISH: '[BEGIN-EN]', LinguaLanguage.SPANISH: '[BEGIN-ES]'}\n",
    "\n",
    "\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "outputs_split_by_lang = {}\n",
    "for l in supported_languages:        \n",
    "    outputs_split_by_lang[l] = open(monolingual_token_file_names[l], 'w')\n",
    "segmented_output = open(f'{MODEL_PATH}/train_segmented.txt', 'w')\n",
    "    \n",
    "# instead of batch iterator, split first by END_TOKEN, and then split_n_detect on fragments containing\n",
    "# multiple END_TOKENs, split by END_TOKEN\n",
    "for buffered_lines in tqdm(batch_iterator(open(f'{TRAIN_DATA_PATH}/train_tiny.txt'), 10000)):        \n",
    "    joined_line = ''.join(buffered_lines)\n",
    "    for detection in lang_detector.split_n_detect(joined_line, END_TOKEN): \n",
    "        # print(detection) \n",
    "        monolingual_fragment = joined_line[detection.start_index: detection.end_index].strip()\n",
    "        # print(monolingual_fragment)\n",
    "        print(f\"{lang_to_token_map[detection.language]} {monolingual_fragment}\") \n",
    "        # outputs_split_by_lang[detection.language].write(monolingual_fragment + '\\n')\n",
    "        segmented_output.write(f\"{lang_to_token_map[detection.language]} {monolingual_fragment}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e39e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "END_TOKEN = '[END]'\n",
    "PROMPT_TOKEN = '[PROMPT]'\n",
    "\n",
    "def delimited_stories_iterator(text):\n",
    "    start = 0\n",
    "    last_match_end = text.find(END_TOKEN)\n",
    "    while last_match_end != -1:\n",
    "        yield text[start:last_match_end + len(END_TOKEN)]\n",
    "        start = last_match_end + len(END_TOKEN)\n",
    "        last_match_end = text.find(END_TOKEN, start)\n",
    "delimited_stories = list(delimited_stories_iterator\n",
    "                         ('foo bar [END] baz [END] quux [END]'))\n",
    "assert delimited_stories == ['foo bar [END]', ' baz [END]', ' quux [END]']\n",
    "\n",
    "\n",
    "paragraph_splitter = re.compile('[^\\n]+')\n",
    "\n",
    "class HackyCheatingLangDetector:\n",
    "    def __init__(self, supported_langs):\n",
    "        pass\n",
    "\n",
    "    def _detect_single_story(self, story_text) -> List[DetectionResult]:\n",
    "        assert story_text.startswith(PROMPT_TOKEN)\n",
    "        task = story_text.split(' ', 2)[1]\n",
    "        assert task in ['english', 'español', 'translate']\n",
    "        if task == 'english':\n",
    "            return [DetectionResult(0, len(story_text), -1, LinguaLanguage.ENGLISH)]\n",
    "        elif task == 'español':\n",
    "            return [DetectionResult(0, len(story_text), -1, LinguaLanguage.SPANISH)]\n",
    "        elif task == 'translate':\n",
    "            paragraph_starts = [m.span()[0] for m in re.finditer(paragraph_splitter, story_text)]\n",
    "            ret = []\n",
    "            for paragraph_no, paragraph_start in paragraph_starts:\n",
    "                \n",
    "                lang = LinguaLanguage.ENGLISH if paragraph_no % 2 == 0 else LinguaLanguage.SPANISH\n",
    "                ret.append(DetectionResult(lang, ))\n",
    "        \n",
    "            \n",
    "\n",
    "    def split_n_detect(self, text, sep) -> List[DetectionResult]:\n",
    "        assert sep == '[END]'\n",
    "        for story_start_index, story_text in delimited_stories_iterator(text):\n",
    "            # _detect_single_story + adjust (add indexes) to be global to text rather than local to story_text\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9ce4fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '\\n', '\\n\\n', '\\n', '\\n\\n\\n', '\\n']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "splits = paragraph_splitter.split('foo\\nbar\\n\\nbaz\\nquux\\n\\n\\netc\\n')\n",
    "print(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7495a73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 9, 13, 20]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[m for m in paragraph_splitter.finditer('\\n+', \"foo\\nbar\\n\\nbaz\\netc\")]\n",
    "[m.span()[0] for m in paragraph_splitter.finditer('foo\\nbar\\n\\nbaz\\nquux\\n\\n\\netc\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8aad6bde-9398-430e-a454-294507e0d2a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 165.45it/s]\n"
     ]
    }
   ],
   "source": [
    "special_tokens = ['[PROMPT]', '[USER]', '[END]', '[PAD]', '[BEGIN-EN]', '[BEGIN-ES]']\n",
    "ENGLISH, SPANISH = 0, 1\n",
    "lang_to_token_map = {LinguaLanguage.ENGLISH: '[BEGIN-EN]', LinguaLanguage.SPANISH: '[BEGIN-ES]'}\n",
    "lingua_lang_to_dense_lang_id = {LinguaLanguage.ENGLISH: 0, LinguaLanguage.SPANISH: 1}\n",
    "supported_languages = [LinguaLanguage.ENGLISH, LinguaLanguage.SPANISH]\n",
    "monolingual_token_file_names = {l: f'{MODEL_PATH}/train_tokenizer_{l}.txt' for l in supported_languages}\n",
    "\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "\n",
    "if DO_TOKENIZATION:\n",
    "    lang_detector = LanguageDetector(supported_languages)\n",
    "\n",
    "    os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "    outputs_split_by_lang = {}\n",
    "    for l in supported_languages:        \n",
    "        outputs_split_by_lang[l] = open(monolingual_token_file_names[l], 'w')\n",
    "    segmented_output = open(f'{MODEL_PATH}/train_segmented.txt', 'w')\n",
    "        \n",
    "    # instead of batch iterator, split first by END_TOKEN, and then split_n_detect on fragments containing\n",
    "    # multiple END_TOKENs, split by END_TOKEN\n",
    "    for buffered_lines in tqdm(batch_iterator(open(f'{TRAIN_DATA_PATH}/train_tiny.txt'), 10000)):        \n",
    "        joined_line = ''.join(buffered_lines)\n",
    "        for detection in lang_detector.split_n_detect(joined_line, END_TOKEN):  \n",
    "            monolingual_fragment = joined_line[detection.start_index: detection.end_index] \n",
    "            # outputs_split_by_lang[detection.language].write(monolingual_fragment + '\\n')\n",
    "            segmented_output.write(f\"{lang_to_token_map[detection.language]} {monolingual_fragment}'\\n\")\n",
    "    \n",
    "    for l in supported_languages:\n",
    "        outputs_split_by_lang[l].close()\n",
    "    segmented_output.close()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0807b784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language.SPANISH [PROMPT] español [USER] Un niño pequeño jugó en el parque. Él vio un perro grande. \"Hola\", dijo el niño. El perro movió la cola. \"¡Qué suerte!\", dijo el niño. \"Puedo jugar contigo\".\n",
      "\n",
      "El niño tiró la pelota. El perro la persiguió. ¡Fue divertido! El niño corrió. El perro corrió. ¡Ellos eran amigos! \n",
      "\n",
      "El niño vio un carro. ¡Era rojo! \"Mira\", dijo el niño al perro. \"¡Ese carro es rojo!\" El perro olfateó. El niño rió. \"¡Qué suerte!\" dijo el niño. \"Puedo jugar más tiempo\".\n",
      "\n",
      "De repente, el carro se detuvo. Una señora salió. \"¡Ese es mi perro!\", dijo la señora. \"¡Podías jugar con él, pero ahora es hora de irnos!\". \n",
      "\n",
      "El niño se puso triste. Él quería jugar más. La señora le dio una galleta. El niño sonrió. \"¡Qué suerte!\", dijo el niño. \"¡Tengo una galleta!\" [END]\n",
      "[PROMPT] español [USER] Una niña llamada Ana cumple años. ¡Hurra! Ana tiene muchísimas amigas que van a su fiesta. ¡Es divertido! Ana abre sus regalos. ¡Wow! Un carro rojo, una pelota grande y... ¡un diamante! ¡Es brillante!\n",
      "\n",
      "\"¡Gracias!\", dice Ana. \"Es muy bonito\". \n",
      "\n",
      "\"De nada\", dice su mamá. \"Espero que te guste\".\n",
      "\n",
      "Ana juega con sus amigas. Se ríen y cantan. Ana está muy feliz. \n",
      "\n",
      "Al final de la fiesta, Ana se acuesta en su cama.  Ella mira el diamante. Es muy brillante.  Ana se siente feliz. Ella tuvo un cumpleaños muy divertido. [END]\n",
      "[PROMPT] español [USER] Una niña pequeña estaba jugando. Ella corría y reía. ¡Era un día divertido! De repente, escuchó un sonido raro. ¡Woo-woo! La niña se asustó. ¡Eran lobos! Los lobos eran grandes y gruñones. La niña lloraba. \n",
      "\n",
      "La niña se escondió detrás de un árbol grande. Los lobos buscaron y buscaron, pero no la encontraron. La niña estaba muy asustada. ¡Pero no era un lobo! Era su papá. ¡Él se puso una máscara de lobo para asustarla!\n",
      "\n",
      "La niña dejó de llorar. ¡Se rio! Ella había jugado con su papá. ¡Era un juego divertido! El papá de la niña le dio un gran abrazo. Él la amaba mucho. \n",
      "\n",
      "La niña y su papá se fueron a casa. Se hicieron galletas \n",
      "Language.ENGLISH he. ¡Fue \n",
      "Language.SPANISH a divertido, incluso con los lobos! [END]\n",
      "[PRO\n",
      "Language.ENGLISH english [USER] One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "\n",
      "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
      "\n",
      "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together. [END]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = open(f'{TRAIN_DATA_PATH}/train_tiny.txt').read()\n",
    "for detection in lang_detector.split_n_detect(sample, '[END]'):\n",
    "    print(str(detection.language) + ' '+ sample[detection.start_index: detection.end_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8012fdd8-a814-4f7c-a550-8d0f50870fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers_by_dense_lang_id = []\n",
    "\n",
    "if DO_TOKENIZATION:\n",
    "    for l in supported_languages:\n",
    "        tokenizer = tokenizers.ByteLevelBPETokenizer()\n",
    "        tokenizer.train(\n",
    "            files=[monolingual_token_file_names[l]], \n",
    "            vocab_size=vocab_size, \n",
    "            min_frequency=2,\n",
    "            special_tokens=special_tokens\n",
    "        )\n",
    "        tokenizer.add_special_tokens(special_tokens)\n",
    "        tokenizer.save_model(MODEL_PATH, f'tiny-stories-{l}-bpe')\n",
    "        tokenizers_by_dense_lang_id.append(tokenizer)\n",
    "else:\n",
    "    for l in supported_languages:\n",
    "        tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "            f'{MODEL_PATH}/tiny-stories-{l}-bpe-vocab.json', \n",
    "            f'{MODEL_PATH}/tiny-stories-{l}-bpe-merges.txt'\n",
    "        )\n",
    "        tokenizer.add_special_tokens(special_tokens)\n",
    "        tokenizers_by_dense_lang_id.append(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad25c70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TOK_ID = trained_tokenizers_by_dense_lang_id[0].token_to_id('[PROMPT]')\n",
    "USER_TOK_ID = trained_tokenizers_by_dense_lang_id[0].token_to_id('[USER]')\n",
    "END_TOK_ID = trained_tokenizers_by_dense_lang_id[0].token_to_id('[END]')\n",
    "PAD_TOK_ID = trained_tokenizers_by_dense_lang_id[0].token_to_id('[PAD]')\n",
    "BEGIN_EN_TOK_ID = trained_tokenizers_by_dense_lang_id[0].token_to_id('[BEGIN-EN]')\n",
    "BEGIN_ES_TOK_ID = trained_tokenizers_by_dense_lang_id[0].token_to_id('[BEGIN-ES]')\n",
    "\n",
    "LANG_BEGIN_TOK_IDS = [BEGIN_EN_TOK_ID, BEGIN_ES_TOK_ID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bbcb12e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   4,    0, 1463,  226,    1,  511,  363,   17,  264,  419,  513,  431,\n",
       "          264, 1126,   19,  424,  286,  409,  426,  270,  369,  794, 4451,   19,\n",
       "            5,  351,  443,   17,  288,  324,  431,  427,  288,  640,   19,  470,\n",
       "          353,  414,  314,  410,  876, 1861,   19,    4,   56, 2770,  413,  712,\n",
       "          948,  973,  226,    2], dtype=torch.int16),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0], dtype=torch.int16))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_splitter_re = re.compile(r'\\[BEGIN-(EN|ES)\\]')\n",
    "lang_code_to_lang_dense_id = {'EN': ENGLISH, 'ES': SPANISH}\n",
    "\n",
    "def multitokenize_delimited_story(text):\n",
    "    assert text.startswith('[BEGIN-EN]') or text.startswith('[BEGIN-ES]')\n",
    "    assert text.endswith(END_TOKEN)\n",
    "\n",
    "    # This splits text into sequences of [lang_code_1, text_1, lang_code_2, text_2, ...]\n",
    "    # where lang_code_i is either 'EN' or 'ES'\n",
    "    per_lang_splits = [p.strip() for p in lang_splitter_re.split(text) if p.strip()]\n",
    "    assert len(per_lang_splits) % 2 == 0\n",
    "    lang_dense_id_per_text = [lang_code_to_lang_dense_id[code] for code in per_lang_splits[::2]]\n",
    "    monolingual_texts = per_lang_splits[1::2]\n",
    "\n",
    "    token_ids, lang_ids = [], []\n",
    "    for lang_dense_id, monolingual_text in zip(lang_dense_id_per_text, monolingual_texts):\n",
    "        tokenizer = tokenizers_by_dense_lang_id[lang_dense_id]\n",
    "        \n",
    "        these_tokens = [LANG_BEGIN_TOK_IDS[lang_dense_id]] + tokenizer.encode(monolingual_text).ids \n",
    "        token_ids.extend(these_tokens)\n",
    "        lang_ids.extend([lang_dense_id] * len(these_tokens))\n",
    "    \n",
    "    return torch.tensor(token_ids, dtype=torch.short), torch.tensor(lang_ids, dtype=torch.short)\n",
    "test_input = (\n",
    "    \"\"\"[BEGIN-EN] [PROMPT] translate [USER] One day, a little boy saw a cake. It was very big and had many strawberries.\n",
    "[BEGIN-ES] Un día, un niño pequeño vio un pastel. Era muy grande y tenía muchas fresas.\n",
    "[BEGIN-EN] Slightly more text [END]\"\"\")\n",
    "# multitokenize_delimited_story(test_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f28087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [01:09<00:00, 1447.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# this is what needs to be replaced \n",
    "\n",
    "if DO_TOKENIZATION:\n",
    "    # stories = chain(tinystories_ds['train']['text'], tinystories_sp_ds['train']['story'])\n",
    "    stories = tinystories_sp_ds['train']\n",
    "    \n",
    "    if not os.path.isdir('tokenized'):\n",
    "        os.mkdir('tokenized')\n",
    "    output_buf = []\n",
    "    num_outputs = 0\n",
    "    for story in tqdm(stories):\n",
    "        encoded_en = torch.tensor([START_TOK, BEGIN_EN_TOK] + \n",
    "                                tokenizer_en.encode(story['translation']).ids + \n",
    "                                [END_TOK], dtype=torch.short)\n",
    "        encoded_es = torch.tensor([START_TOK, BEGIN_ES_TOK] + \n",
    "                                tokenizer_es.encode(story['story']).ids + \n",
    "                                [END_TOK], dtype=torch.short)\n",
    "\n",
    "\n",
    "        en_lang_idxs = torch.zeros_like(encoded_en)\n",
    "        es_lang_idxs = torch.ones_like(encoded_es)\n",
    "        \n",
    "        output_buf.append([encoded_en, en_lang_idxs])\n",
    "        output_buf.append([encoded_es, es_lang_idxs])\n",
    "\n",
    "        if len(output_buf) > 500_000:\n",
    "            torch.save(output_buf, f'tokenized/tokenized-{num_outputs}.pt')\n",
    "            num_outputs += 1\n",
    "            output_buf = []\n",
    "    if output_buf:\n",
    "        torch.save(output_buf, f'tokenized/tokenized-{num_outputs}.pt')\n",
    "        num_outputs += 1\n",
    "        output_buf = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d27547a-ec20-40db-ab71-7dfd168330b7",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fe0f4727-b533-4f9a-a5ff-f7498c41659b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sharded_story(shard_no):\n",
    "    return torch.load(f'tokenized/tokenized-{shard_no}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5f43fb8b-987f-4d14-9712-ba2758335308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:12<00:00, 12.61s/it]\n"
     ]
    }
   ],
   "source": [
    "# load the tokenized stories in parallel using threads\n",
    "# this is faster than loading them sequentially\n",
    "num_shards = len(os.listdir('tokenized'))\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "with ThreadPoolExecutor() as pool:\n",
    "    stories = list(tqdm(pool.map(load_sharded_story, range(num_shards)), total=num_shards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f4189c06-523e-46aa-a571-dd4bc149ec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stories = []\n",
    "for story in stories:\n",
    "    all_stories.extend(story)\n",
    "random.seed(1337)\n",
    "random.shuffle(all_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9b696dc3-de6f-42e7-9095-a96a0912ff80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in stories:  200000\n",
      "length of stories in tokens 400000\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in stories: \", len(all_stories))\n",
    "print(\"length of stories in tokens\", sum(len(story) for story in all_stories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "587a53cb-a587-4013-937e-b0d1ba02976b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# stories longer than 256 : 10705 out of 1000000, 1.07%\n"
     ]
    }
   ],
   "source": [
    "num_stories_to_check = 1_000_000\n",
    "num_long = sum(len(story[0]) > T for story in all_stories[:num_stories_to_check])\n",
    "print(\n",
    "    f\"# stories longer than {T} : {num_long} out of {num_stories_to_check}, {num_long/num_stories_to_check:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e85b5b98-6e94-40e5-9a4f-5677d6124e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola encoded in english\n",
      "[44, 1798]\n",
      "Hola encoded in spani.tqdmsh\n",
      "[580]\n",
      " didn\n",
      "vocab size:  8192\n",
      "first story decoded:  One day, a little boy was playing. He loved to play! Suddenly, knock, knock, knock! Someone knocked on the door. \"Hello!\" said the boy. \"You are a visitor,\" said the boy. The visitor said, \"Hello! I have a gift for you.\" The boy was very happy. \"Thank you!\" he said.\n",
      "\n",
      "The visitor gave the boy a big gift. It was a toy truck! The truck was really cool. It had big wheels and a horn that honked. The boy jumped into the truck and started to play. It was the best day of his life!\n",
      "\n",
      "The boy played and played with the truck. He took it everywhere. He loved the truck. Suddenly, the truck started making a strange noise. Crack, crack, crack! The boy got scared. The truck broke. How sad!\n",
      "\n",
      "The boy started to cry. It really hurt him that his truck was broken! The visitor saw him and said, \"Don’t worry!\" The visitor waved his hand. Suddenly, the truck was new again! The truck was fixed!\n",
      "\n",
      "The boy was very happy. The truck was really cool again! The boy hugged the visitor. He was the best visitor in the world! The boy learned that sometimes things break, but they can also be fixed.\n"
     ]
    }
   ],
   "source": [
    "def encode(text, language_id):\n",
    "    if language_id == 0:\n",
    "        tokenizer = tokenizer_en\n",
    "    else:\n",
    "        tokenizer = tokenizer_es\n",
    "    return tokenizer.encode(text).ids\n",
    "def decode(encoded_text, language_id):\n",
    "    if language_id == 0:\n",
    "        tokenizer = tokenizer_en\n",
    "    else:\n",
    "        tokenizer = tokenizer_es\n",
    "    return tokenizer.decode(encoded_text)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "print(\"Hola encoded in english\")\n",
    "hello_encoded = encode(\"Hola\", 0)\n",
    "print(hello_encoded)\n",
    "\n",
    "print(\"Hola encoded in spani.tqdmsh\")\n",
    "hello_encoded = encode(\"Hola\", 1)\n",
    "print(hello_encoded)\n",
    "print(decode(hello_encoded, 0))\n",
    "\n",
    "vocab_size = tokenizer_en.get_vocab_size()\n",
    "print(\"vocab size: \", vocab_size)\n",
    "token_ids = 2\n",
    "print('first story decoded: ', decode(all_stories[token_ids][0].tolist(), language_id=all_stories[token_ids][1][0]))\n",
    "# PADDING_TOKEN_IDX= encode(\" \", 0)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3d0a7094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,    3,  552,  273,  265, 1008,  403,   18,  272,  636,  273,  963,\n",
       "          18,  407,  310,  273,  490,    5,  312,  577, 1008, 3005,   18,  203,\n",
       "         203,  268,  310,  367,  265,  326,   18,  272,  326,  273,  354,   18,\n",
       "         272,  326,  356,  265, 1212,  969,    5,  272,  310,  297,   16,  285,\n",
       "         570,   16,  326,  311,  203,  203,  268,  326, 1023,   18,  272,  326,\n",
       "         273, 1149,    5,  272,  310,  603,  267,  326,  265,  648,   18,  272,\n",
       "         326,  577,  267,  648,   18,  203,  203,  268,  310,  392,  494,   18,\n",
       "         272,  310,  395,  357,  658,  265,  578,   18,  272,  578,  273,  963,\n",
       "           5,  272,  310,  619,  267,  578,   18,  312,  577,  267,  578,    5,\n",
       "         203,  203,  552,  273, 4317,   18,  272,  310,  273,  351,    5,  272,\n",
       "         310,  356,  538,  545,   18,  437,  400,  587,  765,  276,  319,  640,\n",
       "           5,    1], device='cuda:0', dtype=torch.int16)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_stories[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a832f1a6-9afb-4125-81c4-b1e1457b0071",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(all_stories))\n",
    "\n",
    "train_data = all_stories[:n]\n",
    "val_data = all_stories[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ef2b26ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([   0,    3,  442,   16,  384, 1594,  349,  265,  645,   18,  272,  645,\n",
       "          273,  354,   16,  339,  265,  354,  831,  303,  888, 2843,   18,  781,\n",
       "          712,  276,  319,  349,  267,  645,    5,  203,  203,  486,  403,   16,\n",
       "          267, 1082,  273, 3655,  337, 1110,   18,  272, 1082,  658,  265,  559,\n",
       "          419,  903,    5,  272,  645, 2564,   16,  303,  267,  831,  811,   18,\n",
       "          203,  203,  268,  831,  811,  303,  267,  645,  755,  607,    5,  272,\n",
       "          645,  755,  349,  267, 1082,   18,  272,  645,  755,  337,  802,    5,\n",
       "          203,  203,  552,  273, 1086,    5,  272,  645, 1063,  303, 1063,   16,\n",
       "          303,  846, 2578,  360, 1283,    5,  203,  203,  268,  645,  273,  349,\n",
       "          265,  629, 1052,   18,  407, 1008,  303,  982, 1052,    5,  333,  273,\n",
       "          265,  629, 1052,  276, 2309,    5,    1], device='cuda:0',\n",
       "        dtype=torch.int16),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0], device='cuda:0', dtype=torch.int16)]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e62fd701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = 3\n",
    "\n",
    "a=[1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "a[:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1b29c76d-2e50-47a0-bb79-3a8d7819071d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   0,    3, 4639,  203,   37], device='cuda:0')\n",
      "tensor([   3, 4639,  203,   37,  390], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "  \n",
      "A little dog barks. Woof! Woof! He sees rocks. They are big! He wants to play. He goes to the rocks!\n",
      "\n",
      "The dog looks for another dog. He barks! But there is no other dog! He is alone! The dog feels sad.\n",
      "\n",
      "He sees something. It’s a cat! The cat is on the rocks! The dog is happy. He has a friend!\n",
      "\n",
      "The cat looks at the dog. The cat doesn’t want to play. He wants to sleep! The dog feels sad. He is alone again!\n",
      "\n",
      "The dog leaves. He goes to find another friend. He wants to play! He doesn’t want to be alone!  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, len(data), (B,)) \n",
    "    # ix = [i for i in range(B)]\n",
    "    \n",
    "    x = torch.full((B, T), PAD_TOK, dtype=torch.long)\n",
    "    y = torch.full((B, T), PAD_TOK, dtype=torch.long)\n",
    "    x_lang_idxs = torch.full((B, T), 0, dtype=torch.long)\n",
    "    y_lang_idxs = torch.full((B, T), 0, dtype=torch.long)\n",
    "\n",
    "    for sequence_index, random_story_index in enumerate(ix):\n",
    "        story_tokens = data[random_story_index][0].long()[:T - 1]\n",
    "        story_length = story_tokens.shape[0]\n",
    "        story_lang_idxs = data[random_story_index][1].long()[:T - 1]\n",
    "        # print(story_lang_idxs)\n",
    "        x[sequence_index][0:story_length-1] = story_tokens[0:story_length-1]\n",
    "        y[sequence_index][0:story_length-1] = story_tokens[1:story_length]\n",
    "\n",
    "        last_token_lang_idx =  story_lang_idxs[-1]\n",
    "        x_lang_idxs[sequence_index] = torch.full_like(x_lang_idxs[sequence_index], last_token_lang_idx)\n",
    "        y_lang_idxs[sequence_index] = torch.full_like(y_lang_idxs[sequence_index], last_token_lang_idx)\n",
    "        x_lang_idxs[sequence_index][0: story_lang_idxs.shape[0]-1] = story_lang_idxs[1:story_lang_idxs.shape[0]]\n",
    "        y_lang_idxs[sequence_index][0: story_lang_idxs.shape[0]-1] = story_lang_idxs[0:story_lang_idxs.shape[0]-1]\n",
    "\n",
    "\n",
    "    return x, y, x_lang_idxs, y_lang_idxs\n",
    "\n",
    "xb, yb, x_lang_idxs, y_lang_idxs = get_batch('train')\n",
    "\n",
    "# print(xb[0])\n",
    "# print(yb[0])\n",
    "\n",
    "print(xb[0][:5])\n",
    "print(yb[0][:5])\n",
    "print(x_lang_idxs[0][:5])\n",
    "print(y_lang_idxs[0][:5])\n",
    "language = x_lang_idxs[0,0]\n",
    "print(language)\n",
    "print(decode(xb[0].tolist(), language.item())) # the zero on xb is the first story. \n",
    "\n",
    "\n",
    "y_lang_idxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855ee805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1a3ed8ea-281b-45fa-928a-b856abc13d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    '''One Head of self-attention'''\n",
    "    def __init__(self, H):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(C, H, bias=False)\n",
    "        self.key = nn.Linear(C, H, bias=False)\n",
    "        self.value = nn.Linear(C, H, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(T, T)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        query_vectors = self.query(x)\n",
    "        key_vectors = self.key(x)\n",
    "\n",
    "\n",
    "        # Attention masking(so we can't look into the past):\n",
    "        tril = self.tril\n",
    "        wei = torch.zeros(T, T) \n",
    "        wei = wei.masked_fill(tril == 0, float('-inf')) # set the upper triangular to -inf\n",
    "\n",
    "        # multiply the two to get the attention weights\n",
    "        attention_pattern = query_vectors @ key_vectors.transpose(-2, -1) # T, T\n",
    "        attention_pattern = attention_pattern / (H ** 0.5) # scale the attention pattern for numerical stability\n",
    "        attention_weights = F.softmax(attention_pattern + wei, dim=-1) # T, T (the row dimension is the query)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        value_vectors = self.value(x) # the direction we should go in the embedding space for each token (ie more blue) T, H\n",
    "\n",
    "        # apply the attention weights to the value vectors\n",
    "        context = attention_weights @ value_vectors # T, H\n",
    "\n",
    "        # project back into original space from value space\n",
    "        return context\n",
    "\n",
    "x = torch.randn(B,T,C)\n",
    "head = Head(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6a1ddfac-31a8-43b6-ac7f-646dbea39fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''Multiple heads of self-attention'''\n",
    "    def __init__(self, H, C, n_heads): # H is head embedding space size, n_heads is number of heads\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(H) for _ in range(n_heads)])\n",
    "        self.combine_heads = nn.Linear(H*n_heads, C)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        x = self.combine_heads(x)  # T, C\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e64c34e5-ad29-440c-ab9c-0b8c4c153087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 32])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head = MultiHeadAttention(H, C, n_heads)\n",
    "head.heads[0].forward(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d0beeabe-00ec-45c8-9911-366fec57a5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    '''Feed-forward neural network'''\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(C, C * feedforward_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(C * feedforward_factor, C),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "450533d3-26cb-49f0-afde-777f1b6d8e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    '''Layer normalization'''\n",
    "    def __init__(self, C, use_affine=True):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(C)) if use_affine else None\n",
    "        self.beta = nn.Parameter(torch.zeros(C)) if use_affine else None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        if self.gamma is not None and self.beta is not None:\n",
    "            return self.gamma * (x - mean) / (std + 1e-6) + self.beta\n",
    "        else:\n",
    "            return (x - mean) / (std + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "75b40957-1158-4e85-9b07-6a25d72338e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''Transformer block'''\n",
    "    def __init__(self, H, C, n_heads):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(H, C, n_heads)\n",
    "        self.ff = FeedForward(C)\n",
    "        self.norm1 = LayerNorm(C, use_affine=True)\n",
    "        self.norm2 = LayerNorm(C, use_affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6224a0ef-2693-4e29-b989-3e80a046bd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTokenizerGPT(nn.Module):\n",
    "    def __init__(self, n_layers, n_languages):\n",
    "        super().__init__()\n",
    "        self.token_embedding_tables = [nn.Embedding(vocab_size, C) for _ in range(n_languages)]\n",
    "        self.position_embedding_table = nn.Embedding(T, C)\n",
    "        self.lm_heads = nn.ModuleList([nn.Linear(C, vocab_size) for _ in range(n_languages)])\n",
    "        self.layers = nn.ModuleList([Block(H, C, n_heads) for _ in range(n_layers)])\n",
    "        self.block = nn.ModuleList([Block(H, C, n_heads)])\n",
    "    \n",
    "    # token_lang_idx has same shape as token_ids, encodes langs\n",
    "    def forward(self, token_ids, token_lang_idxs, targets=None):\n",
    "        B, T = token_ids.shape\n",
    "\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T))\n",
    "\n",
    "        token_emb = torch.zeros(B*T, C)\n",
    "\n",
    "        token_ids_flat = token_ids.view(-1)\n",
    "\n",
    "        unique_lang_idxs = torch.unique(token_lang_idxs)\n",
    "\n",
    "        # List of tensors, where each tensor containds the indexes in the flat tokens list of all tokens of a language.\n",
    "        # there are as many tensors as there are languages present in the batch. Total length of all tensors = # tokens\n",
    "        lang_tok_mapping = [torch.where(token_lang_idxs.view(-1) == idx) for idx in unique_lang_idxs]\n",
    "\n",
    "\n",
    "        for lang_id, monolingual_tokens_idx in zip(unique_lang_idxs, lang_tok_mapping):\n",
    "            lang_embedding_table = self.token_embedding_tables[lang_id]\n",
    "            monolingual_tok_emb = lang_embedding_table(token_ids_flat[monolingual_tokens_idx])\n",
    "            token_emb[monolingual_tokens_idx] = monolingual_tok_emb\n",
    "        \n",
    "\n",
    "\n",
    "        x = token_emb.view(B, T, C) + pos_emb # token identities and positions contained\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "\n",
    "        x_flat = x.view(B*T, C)\n",
    "        logits = torch.zeros(B*T, vocab_size)\n",
    "\n",
    "        for lang_id, monolingual_tokens_idx in zip(unique_lang_idxs, lang_tok_mapping):\n",
    "            lm_head = self.lm_heads[lang_id]\n",
    "            new_logits = lm_head(x_flat[monolingual_tokens_idx])\n",
    "            logits[monolingual_tokens_idx] = new_logits\n",
    "\n",
    "        logits = logits.view(B,T,vocab_size)\n",
    "\n",
    "\n",
    "        batch_dim, sequence_dim, embedding_dim = logits.size()\n",
    "\n",
    "        if targets is None:\n",
    "            return logits, None\n",
    "        else:\n",
    "            logits_loss_view = logits.view(-1, vocab_size) \n",
    "            targets_loss_view = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits_loss_view, targets_loss_view)\n",
    "            return logits, loss\n",
    "\n",
    "        return idx\n",
    "    def prompt_model(self, prompt, max_new_tokens, language=\"spanish\", temperature=0.5):\n",
    "        language_token = None\n",
    "        language_id = 0\n",
    "        if language == \"english\":\n",
    "            language_token = BEGIN_EN_TOK\n",
    "            language_id = 0\n",
    "        elif language == \"spanish\":\n",
    "            language_token = BEGIN_ES_TOK\n",
    "            language_id = 1\n",
    "        else:\n",
    "            raise ValueError(f\"invalid language {language}\")\n",
    "        \n",
    "        token_lang_idxs = torch.full([1, T], language_id)\n",
    "\n",
    "        autoregressive_seq = [START_TOK, language_token]\n",
    "\n",
    "        autoregressive_seq += encode(prompt, language_id)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            prediction_index = len(autoregressive_seq)-1\n",
    "\n",
    "            model_input = torch.tensor(autoregressive_seq)\n",
    "            \n",
    "            while model_input.shape[0] < T:\n",
    "                pad_token = torch.tensor([PAD_TOK], dtype=torch.long)\n",
    "                model_input = torch.cat((model_input, pad_token), dim=0)\n",
    "\n",
    "            model_input = model_input.unsqueeze(0)\n",
    "\n",
    "            logits, loss = model.forward(\n",
    "                token_ids=model_input,\n",
    "                token_lang_idxs=token_lang_idxs\n",
    "                )\n",
    "            prediction_token = logits[:, prediction_index, :] / temperature\n",
    "            probabilities = F.softmax(prediction_token, dim=-1)\n",
    "            next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "            next_token = next_token.item()\n",
    "            if(next_token == END_TOK):\n",
    "                break\n",
    "\n",
    "            autoregressive_seq.append(next_token)\n",
    "        # get the autoregressive sequence\n",
    "        return decode(autoregressive_seq, language_id)\n",
    "\n",
    "\n",
    "model = MultiTokenizerGPT(n_layers, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "34e95468-ce3b-4351-9355-2859a0d52979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256, 8192])\n",
      "tensor(9.2287, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "xb, yb, x_lang_idxs, y_lang_idxs = get_batch('train')\n",
    "\n",
    "logits, loss = model(\n",
    "    token_ids = xb,\n",
    "    token_lang_idxs = x_lang_idxs,\n",
    "    targets = yb\n",
    ")\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# test_idx = torch.zeros(1, T).long()\n",
    "# model.forward(idx=test_idx)\n",
    "# decode(model.generate(idx=test_idx, max_new_tokens=100)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d70449a1-8920-40e5-8c78-6e9289c1703b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters in the model (millions):  12.825856\n"
     ]
    }
   ],
   "source": [
    "# get the number of parameters in the model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"number of parameters in the model (millions): \", count_parameters(model) /1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "39127ecd-0a25-4817-9fe1-e0d6fef45f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]], device='cuda:0')"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = torch.zeros(1, 1).long()\n",
    "token_ids[:,-T:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "20d745fa-d48c-441c-886e-890c2a7636aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ae2684ea-3417-4708-b49d-03acf0e1dc9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor(2.3641, device='cuda:0'),\n",
       " 'val': tensor(2.3611, device='cuda:0')}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_iters = 10\n",
    "eval_interval = 300\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(is_last=False):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        real_iters = eval_iters\n",
    "        if is_last and split == 'val':  # increase last eval to mitigate noise\n",
    "            real_iters *= 10 \n",
    "        losses = torch.zeros(real_iters)\n",
    "        for k in range(real_iters):\n",
    "            # X, Y = get_batch(split)\n",
    "            xb, yb, x_lang_idxs, y_lang_idxs = get_batch(split)\n",
    "\n",
    "            logits, loss = model(\n",
    "                token_ids = xb,\n",
    "                token_lang_idxs = x_lang_idxs,\n",
    "                targets = yb\n",
    "            )\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean() / chars_per_token\n",
    "    model.train()\n",
    "    return out\n",
    "estimate_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c2b37613-849e-46a5-9e67-64c432bd9ebd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/60000 [00:01<7:41:08,  2.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 2.1513853073120117, 'val': 2.1557843685150146}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 302/60000 [00:49<4:41:38,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.5170854330062866, 'val': 0.5216456651687622}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 602/60000 [01:35<5:16:23,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.43009981513023376, 'val': 0.4364422559738159}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 902/60000 [02:18<4:39:22,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.401716411113739, 'val': 0.4039750099182129}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1202/60000 [03:06<5:08:14,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.3641626834869385, 'val': 0.3777467608451843}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1501/60000 [03:50<5:54:29,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.35846540331840515, 'val': 0.36740297079086304}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1802/60000 [04:38<5:43:38,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.34140029549598694, 'val': 0.3493606746196747}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 2102/60000 [05:26<3:51:38,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.33492937684059143, 'val': 0.34615659713745117}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2402/60000 [06:00<3:52:48,  4.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.32648611068725586, 'val': 0.32709553837776184}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 2702/60000 [06:46<3:51:33,  4.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.3109878599643707, 'val': 0.3214492201805115}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 3000/60000 [07:33<1:39:52,  9.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.31846824288368225, 'val': 0.31900450587272644}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3301/60000 [08:19<7:07:56,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.29730021953582764, 'val': 0.3159204125404358}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3602/60000 [09:05<5:02:30,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.3038983941078186, 'val': 0.31586307287216187}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 3902/60000 [09:50<4:53:07,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.29337626695632935, 'val': 0.3096903860569}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 4202/60000 [10:34<4:37:02,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.2964930534362793, 'val': 0.3065257966518402}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4502/60000 [11:11<4:54:17,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.2910471558570862, 'val': 0.30113714933395386}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4802/60000 [11:52<4:36:08,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.28474366664886475, 'val': 0.3030562996864319}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 5102/60000 [12:42<3:52:09,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.2877989709377289, 'val': 0.30258703231811523}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 5402/60000 [13:26<4:50:57,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.27914828062057495, 'val': 0.29331788420677185}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 5702/60000 [14:20<5:04:13,  2.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.27915605902671814, 'val': 0.2948143780231476}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 6001/60000 [15:09<5:30:04,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.27634677290916443, 'val': 0.29945164918899536}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 6302/60000 [15:56<4:43:51,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.2820090353488922, 'val': 0.2884598970413208}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 6602/60000 [16:32<4:29:16,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.2691366970539093, 'val': 0.29921823740005493}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6902/60000 [17:24<4:47:35,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.27004218101501465, 'val': 0.28702354431152344}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 7202/60000 [18:12<4:52:26,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.271646112203598, 'val': 0.28759294748306274}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 7502/60000 [18:59<3:29:59,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.27320289611816406, 'val': 0.2884172797203064}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 7802/60000 [19:45<5:04:26,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.26183000206947327, 'val': 0.27706027030944824}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 8102/60000 [20:34<3:32:07,  4.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.26650917530059814, 'val': 0.27785369753837585}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 8402/60000 [21:15<4:57:09,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.26252928376197815, 'val': 0.27968934178352356}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 8702/60000 [22:01<4:34:13,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.26725196838378906, 'val': 0.2759448289871216}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 9001/60000 [22:59<6:19:03,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.2640162408351898, 'val': 0.28430432081222534}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 9302/60000 [23:51<3:27:38,  4.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.26456397771835327, 'val': 0.2844673991203308}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 9602/60000 [24:34<4:31:50,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.2623649835586548, 'val': 0.2692298889160156}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 9902/60000 [25:24<3:23:08,  4.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.2569982707500458, 'val': 0.28546053171157837}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 10202/60000 [26:10<3:19:17,  4.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.25185221433639526, 'val': 0.27724024653434753}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 10502/60000 [26:52<3:19:01,  4.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.2532210648059845, 'val': 0.27779123187065125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 10802/60000 [27:40<3:18:38,  4.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.2553711235523224, 'val': 0.27574536204338074}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 11102/60000 [28:30<4:39:55,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.24981610476970673, 'val': 0.27189043164253235}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 11127/60000 [28:33<2:05:27,  6.49it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[142], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m xb, yb, x_lang_idxs, y_lang_idxs \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# loss\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_lang_idxs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_lang_idxs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[136], line 39\u001b[0m, in \u001b[0;36mMultiTokenizerGPT.forward\u001b[0;34m(self, token_ids, token_lang_idxs, targets)\u001b[0m\n\u001b[1;32m     36\u001b[0m x \u001b[38;5;241m=\u001b[39m token_emb\u001b[38;5;241m.\u001b[39mview(B, T, C) \u001b[38;5;241m+\u001b[39m pos_emb \u001b[38;5;66;03m# token identities and positions contained\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 39\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m x_flat \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(B\u001b[38;5;241m*\u001b[39mT, C)\n\u001b[1;32m     43\u001b[0m logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(B\u001b[38;5;241m*\u001b[39mT, vocab_size)\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[128], line 11\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1514\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1514\u001b[0m     forward_call \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tracing_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward)\n\u001b[1;32m   1515\u001b[0m     \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dump_model_interval = 1000\n",
    "chars_per_token=3.9\n",
    "\n",
    "#print('loading last model')\n",
    "#model.load_state_dict(torch.load('tiny-stories-model-12.pt'))\n",
    "\n",
    "\n",
    "for steps in tqdm(range(0, max_iters)):\n",
    "    xb, yb, x_lang_idxs, y_lang_idxs = get_batch('train')\n",
    "    # loss\n",
    "    logits, loss = model(\n",
    "        token_ids = xb,\n",
    "        token_lang_idxs = x_lang_idxs,\n",
    "        targets = yb\n",
    "    )\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        # wandb.log({\"train\": losses['train'].item(), \"val\": losses['val'].item(), \"l2\":l2})\n",
    "        print({\"train\": losses['train'].item(), \"val\": losses['val'].item()})\n",
    "    if steps % dump_model_interval == 0 and steps > 0:\n",
    "        model_no = steps // dump_model_interval\n",
    "        torch.save(model.state_dict(), f'tiny-stories-model-{model_no}.pt')\n",
    "\n",
    "losses = estimate_loss(is_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7341ae92-9812-447e-9b96-523989708372",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89593204-60c1-43a8-92dd-3e5ac55c4903",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'tiny-stories-multilingual-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5637b3b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load('tiny-stories-model-10.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a71a0ceb-ce02-4910-b814-68b54c0927c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Diego tenia un perro grande. He jumps high in a tall tree. He likes to jump!\\n\\nOne day, Diego sees an insect. It is very cute. \"Hello, insect,\" says Diego. The insect flies away. Diego is sad.\\n\\n\"Don\\'t be sad,\" says Mama Bird. \"The insect won\\'t talk.\"\\nDiego looks at the cat. The cat jumps and sings. \"That is not a good insect,\" says the cat. \"But it is a pretty butterfly!\"\\n\\nDiego'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_idx = torch.zeros(1, T).long() * 198\n",
    "# print(decode(\n",
    "#     model.generate(idx=test_idx, max_new_tokens=C)[0].tolist()\n",
    "# )[T:])\n",
    "model.prompt_model(\n",
    "        prompt=\"Diego tenia un perro grande.\",\n",
    "    max_new_tokens=100,\n",
    "    language=\"english\",\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf25a21",
   "metadata": {},
   "source": [
    "# Multitokenizer Toy Implemetation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be4b43b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221470c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
