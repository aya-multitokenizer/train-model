{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "428d10f1",
   "metadata": {},
   "source": [
    "## Experiment Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da08a5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_TOKENIZATION=True\n",
    "TINY_DATA_LOAD=True # If True, load tiny data for testing\n",
    "TRAIN_MODEL=False\n",
    "\n",
    "DATA_PREFIX='expt_1'\n",
    "# DATA_PREFIX='tiny_sample'\n",
    "MODEL_EXPT_NAME=\"multitok_model_1\"\n",
    "\n",
    "TRAIN_DATA_PATH=f'train-model/{DATA_PREFIX}'\n",
    "MODEL_PATH=f'train-model/{DATA_PREFIX}/{MODEL_EXPT_NAME}'\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"eval_interval\": 300,\n",
    "    \"max_iters\": 60000, \n",
    "    \"H\": 32, # per head dimension size\n",
    "    \"B\": 64, # batch size\n",
    "    \"T\": 256, # Sequence length\n",
    "    \"C\": 256, # model size\n",
    "    \"feedforward_factor\": 3,\n",
    "    \"n_heads\": 8,\n",
    "    \"dropout\": 0.0,\n",
    "    \"l2_penalty\": 0.0,\n",
    "    \"n_layers\": 12,\n",
    "    \"vocab_size\": 2**13,\n",
    "    # \"git_hash\": os.popen(\"git rev-parse HEAD\").read().strip()\n",
    "}\n",
    "\n",
    "for k,v in config.items():\n",
    "    locals()[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7726f1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lingua in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.15.0)\n",
      "Requirement already satisfied: polib>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lingua) (1.2.0)\n",
      "Requirement already satisfied: click>=8.0.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lingua) (8.1.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: multi-tokenizer in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.1.4)\n",
      "Requirement already satisfied: lingua-language-detector<3.0.0,>=2.0.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from multi-tokenizer) (2.0.2)\n",
      "Requirement already satisfied: tokenizers<0.20.0,>=0.19.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from multi-tokenizer) (0.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (0.23.4)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (2024.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (6.0.1)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->multi-tokenizer) (2024.6.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: ipython-unittest in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.3.2)\n",
      "Requirement already satisfied: ipython in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython-unittest) (8.17.2)\n",
      "Requirement already satisfied: decorator in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ipython-unittest) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ipython-unittest) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ipython-unittest) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ipython-unittest) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ipython-unittest) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ipython-unittest) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ipython-unittest) (5.14.3)\n",
      "Requirement already satisfied: exceptiongroup in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ipython-unittest) (1.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ipython-unittest) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jedi>=0.16->ipython->ipython-unittest) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pexpect>4.3->ipython->ipython-unittest) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython->ipython-unittest) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from stack-data->ipython->ipython-unittest) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from stack-data->ipython->ipython-unittest) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from stack-data->ipython->ipython-unittest) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython->ipython-unittest) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install lingua \n",
    "!pip install multi-tokenizer\n",
    "!pip install ipython-unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa17e9b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feadb260-56af-4aa5-951a-55d3a04d157e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ipython_unittest extension is already loaded. To reload it, use:\n",
      "  %reload_ext ipython_unittest\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from itertools import chain\n",
    "import tokenizers\n",
    "    \n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from itertools import islice\n",
    "from typing import List\n",
    "\n",
    "%load_ext ipython_unittest\n",
    "\n",
    "from multi_tokenizer import LanguageDetector\n",
    "from lingua import DetectionResult, Language\n",
    "\n",
    "\n",
    "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_default_device(device)\n",
    "# assert device == 'cuda', \"This notebook is not optimized for CPU\"\n",
    "if device == 'cpu' and TRAIN_MODEL:\n",
    "    print(\"Warning: Training on CPU will be slow\")\n",
    "    assert False\n",
    "\n",
    "if 'train-model' not in sys.path: sys.path.append('train-model')\n",
    "import data_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e920f455",
   "metadata": {},
   "source": [
    "## Tokenization preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14f37cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "END_TOKEN = '[END]'\n",
    "\n",
    "def delimited_stories_iterator(text):\n",
    "    start = 0\n",
    "    last_match_end = text.find(END_TOKEN)\n",
    "    while last_match_end != -1:\n",
    "        yield text[start:last_match_end + len(END_TOKEN)]\n",
    "        start = last_match_end + len(END_TOKEN)\n",
    "        last_match_end = text.find(END_TOKEN, start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91730595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/unittest.status+json": {
       "color": "yellow",
       "message": "",
       "previous": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/unittest.status+json": {
       "color": "lightgreen",
       "message": ".\n----------------------------------------------------------------------\nRan 1 test in 0.000s\n\nOK\n",
       "previous": 0
      },
      "text/plain": [
       "Success"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%unittest_testcase\n",
    "def test_delimited_stories_iterator(self):\n",
    "    test_input = \"\"\"[PROMPT] español [USER] Un niñ [END]\n",
    "[PROMPT] español [USER] Hola [END]\"\"\"\n",
    "    delim_stories = list(delimited_stories_iterator(test_input))\n",
    "    self.assertEqual('[PROMPT] español [USER] Un niñ [END]', delim_stories[0])\n",
    "    self.assertEqual('\\n[PROMPT] español [USER] Hola [END]', delim_stories[1])\n",
    "    self.assertEqual(test_input, ''.join(delim_stories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3fce254",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilePartitionIterator:\n",
    "    def __init__(self, file_obj, delimiter):\n",
    "        self.delimiter = delimiter\n",
    "        self.file = file_obj\n",
    "        self.buffer = \"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            index = self.buffer.find(self.delimiter)\n",
    "            if index != -1:\n",
    "                partition = self.buffer[:index + len(self.delimiter)]\n",
    "                self.buffer = self.buffer[index + len(self.delimiter):]\n",
    "                return partition\n",
    "\n",
    "            chunk = self.file.read(4096)  # Read in chunks of 4KB\n",
    "            if not chunk:\n",
    "                if self.buffer:\n",
    "                    partition = self.buffer\n",
    "                    self.buffer = \"\"\n",
    "                    return partition\n",
    "                self.file.close()\n",
    "                raise StopIteration\n",
    "\n",
    "            self.buffer += chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3951b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/unittest.status+json": {
       "color": "yellow",
       "message": "",
       "previous": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/unittest.status+json": {
       "color": "lightgreen",
       "message": ".\n----------------------------------------------------------------------\nRan 1 test in 0.000s\n\nOK\n",
       "previous": 0
      },
      "text/plain": [
       "Success"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%unittest_testcase\n",
    "import io\n",
    "def test_file_partition_iterator(self):\n",
    "    test_input = \"\"\"[PROMPT] español [USER] Un niñ [END]\n",
    "[PROMPT] español [USER] Hola [END]\"\"\"\n",
    "    test_input_filelike = io.StringIO(test_input)\n",
    "    delim_stories = list(FilePartitionIterator(test_input_filelike, END_TOKEN))\n",
    "    self.assertEqual('[PROMPT] español [USER] Un niñ [END]', delim_stories[0])\n",
    "    self.assertEqual('\\n[PROMPT] español [USER] Hola [END]', delim_stories[1])\n",
    "    self.assertEqual(test_input, ''.join(delim_stories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74fd684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "END_TOKEN = '[END]'\n",
    "PROMPT_TOKEN = '[PROMPT]'\n",
    "\n",
    "\n",
    "class HackyCheatingLangDetector:\n",
    "    def __init__(self, languages: List[Language]) -> None:\n",
    "        pass\n",
    "\n",
    "\n",
    "    def _detect_single_story(self, story_text) -> List[DetectionResult]:\n",
    "        # story_text = story_text.strip()\n",
    "        # assert story_text.startswith(PROMPT_TOKEN), f\"Unexpected start: {story_text[:20]}\"\n",
    "        task = story_text.split(' ', 2)[1]\n",
    "        assert task in ['english', 'español', 'translate']\n",
    "        paragraph_splitter = re.compile('[^\\n]+')\n",
    "        if task == 'english':\n",
    "            return [DetectionResult(0, len(story_text), 0, Language.ENGLISH)]\n",
    "        elif task == 'español':\n",
    "            return [DetectionResult(0, len(story_text), 0, Language.SPANISH)]\n",
    "        elif task == 'translate':\n",
    "            paragraph_starts = [m.span()[0] for m in re.finditer(paragraph_splitter, story_text)]\n",
    "            ret = []\n",
    "            for paragraph_no, paragraph_start in enumerate(paragraph_starts):\n",
    "                next_paragraph_start = paragraph_starts[paragraph_no + 1] if paragraph_no + 1 < len(paragraph_starts) else len(story_text)+1\n",
    "                lang = Language.ENGLISH if paragraph_no % 2 == 0 else Language.SPANISH\n",
    "                ret.append(DetectionResult(paragraph_start, next_paragraph_start, 0, lang))\n",
    "            return ret\n",
    "\n",
    "  \n",
    "    def split_n_detect(self, text: str, sep: str = \" \") -> List[DetectionResult]:\n",
    "        \"\"\"Split Text and Detect Language.\"\"\"\n",
    "\n",
    "        def merge_results(\n",
    "            results: List[List[DetectionResult]],\n",
    "        ) -> List[DetectionResult]:\n",
    "            \"\"\"Merge Results. If consecutive words are detected as the same language, merge them.\"\"\"\n",
    "            merged_results: list[DetectionResult] = []\n",
    "            for result in results:\n",
    "                if not merged_results:\n",
    "                    merged_results.extend(result)\n",
    "                else:\n",
    "                    for detection in result:\n",
    "                        last_result = merged_results[-1]\n",
    "                        if detection.language == last_result.language:\n",
    "                            merged_results[-1] = DetectionResult(\n",
    "                                language=last_result.language,\n",
    "                                start_index=last_result.start_index,\n",
    "                                end_index=last_result.end_index\n",
    "                                + detection.end_index\n",
    "                                - detection.start_index,\n",
    "                                word_count=last_result.word_count\n",
    "                                + detection.word_count,\n",
    "                            )\n",
    "                        else:\n",
    "                            merged_results.append(\n",
    "                                DetectionResult(\n",
    "                                    language=detection.language,\n",
    "                                    start_index=last_result.end_index,\n",
    "                                    end_index=last_result.end_index\n",
    "                                    + detection.end_index\n",
    "                                    - detection.start_index,\n",
    "                                    word_count=detection.word_count,\n",
    "                                )\n",
    "                            )\n",
    "            return merged_results\n",
    "\n",
    "\n",
    "        results = []\n",
    "        # Detect the language of each part\n",
    "        for story_text in delimited_stories_iterator(text):\n",
    "            results.append(self._detect_single_story(story_text))\n",
    "        \n",
    "        # Merge consecutive results with the same language\n",
    "        merged_results = merge_results(results)\n",
    "        return merged_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f79fb5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/unittest.status+json": {
       "color": "yellow",
       "message": "",
       "previous": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/unittest.status+json": {
       "color": "lightgreen",
       "message": ".\n----------------------------------------------------------------------\nRan 1 test in 0.001s\n\nOK\n",
       "previous": 0
      },
      "text/plain": [
       "Success"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%unittest_testcase\n",
    "def test_hacky_cheating_language_detector(self):\n",
    "    test_input = \"\"\"[PROMPT] english [USER] Hi [END]\n",
    "[PROMPT] español [USER] a child [END]\n",
    "[PROMPT] translate [USER] It was night\n",
    "Era de noche\n",
    "Hi\n",
    "Hola [END]\"\"\"\n",
    "    hcld = HackyCheatingLangDetector([])\n",
    "    detections = hcld.split_n_detect(test_input, END_TOKEN)\n",
    "    self.assertEqual(6, len(detections))\n",
    "    expected_detections_with_langs = [\n",
    "        (Language.ENGLISH, '[PROMPT] english [USER] Hi [END]'),\n",
    "        (Language.SPANISH, '\\n[PROMPT] español [USER] a child [END]'),\n",
    "        (Language.ENGLISH, '\\n[PROMPT] translate [USER] It was night'),\n",
    "        (Language.SPANISH, '\\nEra de noche'),\n",
    "        (Language.ENGLISH, '\\nHi'),\n",
    "        (Language.SPANISH, '\\nHola [END]')\n",
    "    ]\n",
    "    for detection, (expected_lang, expected_string) in zip(\n",
    "        detections, expected_detections_with_langs):\n",
    "        self.assertEqual(expected_lang, detection.language)\n",
    "        self.assertEqual(expected_string, test_input[detection.start_index: detection.end_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8aad6bde-9398-430e-a454-294507e0d2a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "special_tokens = ['[PROMPT]', '[USER]', '[END]', '[PAD]', '[BEGIN-EN]', '[BEGIN-ES]']\n",
    "ENGLISH, SPANISH = 0, 1\n",
    "lang_to_token_map = {Language.ENGLISH: '[BEGIN-EN]', Language.SPANISH: '[BEGIN-ES]'}\n",
    "lingua_lang_to_dense_lang_id = {Language.ENGLISH: ENGLISH, Language.SPANISH: SPANISH}\n",
    "supported_languages = [Language.ENGLISH, Language.SPANISH]\n",
    "monolingual_token_file_names = {l: f'{MODEL_PATH}/train_tokenizer_{l}.txt' for l in supported_languages}\n",
    "\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "\n",
    "def detections_to_lang_identified_text(detections: List[DetectionResult], text: str) -> str:\n",
    "    ret = []\n",
    "    for detection in detections:\n",
    "        ret.append(f\"{lang_to_token_map[detection.language]}{text[detection.start_index: detection.end_index]}\")\n",
    "    return ''.join(ret)\n",
    "\n",
    "\n",
    "if DO_TOKENIZATION:\n",
    "    lang_detector = HackyCheatingLangDetector(supported_languages)\n",
    "\n",
    "    os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "    outputs_split_by_lang = {}\n",
    "    for l in supported_languages:        \n",
    "        outputs_split_by_lang[l] = open(monolingual_token_file_names[l], 'w')\n",
    "    segmented_output = open(f'{MODEL_PATH}/train_segmented.txt', 'w')\n",
    "\n",
    "    for single_story in FilePartitionIterator(open(f'{TRAIN_DATA_PATH}/train.txt', 'r'), END_TOKEN):\n",
    "        detections = lang_detector.split_n_detect(single_story, END_TOKEN)              \n",
    "        for detection in detections: \n",
    "            monolingual_fragment = single_story[detection.start_index: detection.end_index]\n",
    "            outputs_split_by_lang[detection.language].write(monolingual_fragment + '\\n')\n",
    "        segmented_output.write(detections_to_lang_identified_text(detections, single_story))  \n",
    "\n",
    "    for l in supported_languages:\n",
    "        outputs_split_by_lang[l].close()\n",
    "    segmented_output.close()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8012fdd8-a814-4f7c-a550-8d0f50870fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizers_by_dense_lang_id = []\n",
    "\n",
    "if DO_TOKENIZATION:\n",
    "    for l in supported_languages:\n",
    "        tokenizer = tokenizers.ByteLevelBPETokenizer()\n",
    "        tokenizer.train(\n",
    "            files=[monolingual_token_file_names[l]], \n",
    "            vocab_size=vocab_size, \n",
    "            min_frequency=2,\n",
    "            special_tokens=special_tokens\n",
    "        )\n",
    "        tokenizer.add_special_tokens(special_tokens)\n",
    "        tokenizer.save_model(MODEL_PATH, f'tiny-stories-{l}-bpe')\n",
    "        tokenizers_by_dense_lang_id.append(tokenizer)\n",
    "else:\n",
    "    for l in supported_languages:\n",
    "        tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "            f'{MODEL_PATH}/tiny-stories-{l}-bpe-vocab.json', \n",
    "            f'{MODEL_PATH}/tiny-stories-{l}-bpe-merges.txt'\n",
    "        )\n",
    "        tokenizer.add_special_tokens(special_tokens)\n",
    "        tokenizers_by_dense_lang_id.append(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad25c70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TOK_ID = tokenizers_by_dense_lang_id[0].token_to_id('[PROMPT]')\n",
    "USER_TOK_ID = tokenizers_by_dense_lang_id[0].token_to_id('[USER]')\n",
    "END_TOK_ID = tokenizers_by_dense_lang_id[0].token_to_id('[END]')\n",
    "PAD_TOK_ID = tokenizers_by_dense_lang_id[0].token_to_id('[PAD]')\n",
    "BEGIN_EN_TOK_ID = tokenizers_by_dense_lang_id[0].token_to_id('[BEGIN-EN]')\n",
    "BEGIN_ES_TOK_ID = tokenizers_by_dense_lang_id[0].token_to_id('[BEGIN-ES]')\n",
    "\n",
    "LANG_BEGIN_TOK_IDS = [BEGIN_EN_TOK_ID, BEGIN_ES_TOK_ID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbcb12e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   4,  226,    0, 1457,  226,    1,  509,  362,   17,  264,  418,  512,\n",
       "          432,  264, 1120,   19,  421,  287,  406,  426,  270,  369,  795, 4445,\n",
       "           19,  204,    5,  383,  442,   17,  288,  325,  432,  428,  288,  637,\n",
       "           19,  470,  355,  416,  311,  410,  874, 1838,   19,  204,    4,  304,\n",
       "         2794,  412,  710,  948,  972,  226,    2], dtype=torch.int16),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0], dtype=torch.int16))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_splitter_re = re.compile(r'\\[BEGIN-(EN|ES)\\]')\n",
    "lang_code_to_lang_dense_id = {'EN': ENGLISH, 'ES': SPANISH}\n",
    "\n",
    "def multi_tokenize_delimited_story(text):\n",
    "    assert text.startswith('[BEGIN-EN]') or text.startswith('[BEGIN-ES]')\n",
    "    # assert text.endswith(END_TOKEN)\n",
    "\n",
    "    # This splits text into sequences of [lang_code_1, text_1, lang_code_2, text_2, ...]\n",
    "    # where lang_code_i is either 'EN' or 'ES'\n",
    "    per_lang_splits = [p for p in lang_splitter_re.split(text) if p.strip()]\n",
    "    assert len(per_lang_splits) % 2 == 0\n",
    "    lang_dense_id_per_text = [lang_code_to_lang_dense_id[code] for code in per_lang_splits[::2]]\n",
    "    monolingual_texts = per_lang_splits[1::2]\n",
    "\n",
    "    token_ids, lang_ids = [], []\n",
    "    for lang_dense_id, monolingual_text in zip(lang_dense_id_per_text, monolingual_texts):\n",
    "        tokenizer = tokenizers_by_dense_lang_id[lang_dense_id]\n",
    "        \n",
    "        these_tokens = [LANG_BEGIN_TOK_IDS[lang_dense_id]] + tokenizer.encode(monolingual_text).ids \n",
    "        token_ids.extend(these_tokens)\n",
    "        lang_ids.extend([lang_dense_id] * len(these_tokens))\n",
    "    \n",
    "    return torch.tensor(token_ids, dtype=torch.short), torch.tensor(lang_ids, dtype=torch.short)\n",
    "test_input = (\n",
    "    \"\"\"[BEGIN-EN] [PROMPT] translate [USER] One day, a little boy saw a cake. It was very big and had many strawberries.\n",
    "[BEGIN-ES] Un día, un niño pequeño vio un pastel. Era muy grande y tenía muchas fresas.\n",
    "[BEGIN-EN] Slightly more text [END]\"\"\")\n",
    "multi_tokenize_delimited_story(test_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "601cc17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEGIN-EN] [PROMPT] translate [USER] One day, a little boy saw a cake. It was very big and had many strawberries.\n",
      "[BEGIN-ES] Un día, un niño pequeño vio un pastel. Era muy grande y tenía muchas fresas.\n",
      "[BEGIN-EN] Slightly more text [END]\n"
     ]
    }
   ],
   "source": [
    "def multi_detokenize(tokens, langs):\n",
    "    ret = \"\"\n",
    "    for token, lang in zip(tokens, langs):\n",
    "        ret += tokenizers_by_dense_lang_id[lang].decode([token], skip_special_tokens=False)\n",
    "    return ret\n",
    "\n",
    "print(multi_detokenize(*multi_tokenize_delimited_story(test_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99f28087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "394635it [03:28, 1893.44it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m num_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m story \u001b[38;5;129;01min\u001b[39;00m tqdm(FilePartitionIterator(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/train_segmented.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m), END_TOKEN)):\n\u001b[0;32m----> 5\u001b[0m     output_buf\u001b[38;5;241m.\u001b[39mappend(\u001b[43mmulti_tokenize_delimited_story\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstory\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(output_buf) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m500_000\u001b[39m:\n\u001b[1;32m      8\u001b[0m         torch\u001b[38;5;241m.\u001b[39msave(output_buf, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/tokenized-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_outputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[16], line 19\u001b[0m, in \u001b[0;36mmulti_tokenize_delimited_story\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lang_dense_id, monolingual_text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(lang_dense_id_per_text, monolingual_texts):\n\u001b[1;32m     17\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m tokenizers_by_dense_lang_id[lang_dense_id]\n\u001b[0;32m---> 19\u001b[0m     these_tokens \u001b[38;5;241m=\u001b[39m [LANG_BEGIN_TOK_IDS[lang_dense_id]] \u001b[38;5;241m+\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonolingual_text\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mids \n\u001b[1;32m     20\u001b[0m     token_ids\u001b[38;5;241m.\u001b[39mextend(these_tokens)\n\u001b[1;32m     21\u001b[0m     lang_ids\u001b[38;5;241m.\u001b[39mextend([lang_dense_id] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(these_tokens))\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tokenizers/implementations/base_tokenizer.py:223\u001b[0m, in \u001b[0;36mBaseTokenizer.encode\u001b[0;34m(self, sequence, pair, is_pretokenized, add_special_tokens)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sequence \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencode: `sequence` can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be `None`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if DO_TOKENIZATION:\n",
    "    output_buf = []\n",
    "    num_outputs = 0\n",
    "    for story in tqdm(FilePartitionIterator(open(f'{MODEL_PATH}/train_segmented.txt', 'r'), END_TOKEN)):\n",
    "        output_buf.append(multi_tokenize_delimited_story(story))\n",
    "\n",
    "        if len(output_buf) > 500_000:\n",
    "            torch.save(output_buf, f'{MODEL_PATH}/tokenized-{num_outputs}.pt')\n",
    "            num_outputs += 1\n",
    "            if num_outputs == 1:\n",
    "                torch.save(output_buf[:1000], f'{MODEL_PATH}/tiny-shard.pt')\n",
    "            output_buf = []\n",
    "           \n",
    "\n",
    "    if output_buf:\n",
    "        torch.save(output_buf, f'{MODEL_PATH}/tokenized-{num_outputs}.pt')\n",
    "        num_outputs += 1\n",
    "        output_buf = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d27547a-ec20-40db-ab71-7dfd168330b7",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f43fb8b-987f-4d14-9712-ba2758335308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 stories\n",
      "length of dataset in stories:  1000\n",
      "length of stories in tokens 195074\n",
      "# stories longer than 256 : 92 out of 1000, 9.20%\n"
     ]
    }
   ],
   "source": [
    "def load_sharded_story(shard_no):\n",
    "    return torch.load(f'{MODEL_PATH}/tokenized-{shard_no}.pt')\n",
    "\n",
    "if TRAIN_MODEL or TINY_DATA_LOAD:\n",
    "    # load the tokenized stories in parallel using threads\n",
    "    # this is faster than loading them sequentially\n",
    "\n",
    "    if not TINY_DATA_LOAD:\n",
    "        num_shards = len(glob.glob(f'{MODEL_PATH}/tokenized-*'))\n",
    "        from concurrent.futures import ThreadPoolExecutor\n",
    "        with ThreadPoolExecutor() as pool:\n",
    "            stories = list(tqdm(pool.map(load_sharded_story, range(num_shards)), total=num_shards))\n",
    "    else:\n",
    "        stories = [torch.load(f'{MODEL_PATH}/tiny-shard.pt')]\n",
    "    print(\"Loaded\", sum(len(story) for story in stories), \"stories\")\n",
    "\n",
    "    all_stories = []\n",
    "    for story in stories:\n",
    "        all_stories.extend(story)\n",
    "    random.seed(1337)\n",
    "    random.shuffle(all_stories)\n",
    "\n",
    "    print(\"length of dataset in stories: \", len(all_stories))\n",
    "    print(\"length of stories in tokens\", sum(len(story[0]) for story in all_stories))\n",
    "\n",
    "    num_stories_to_check = min(1_000_000, len(all_stories))\n",
    "    num_long = sum(len(story[0]) > T for story in all_stories[:num_stories_to_check])\n",
    "    print(\n",
    "        f\"# stories longer than {T} : {num_long} out of {num_stories_to_check}, {num_long/num_stories_to_check:.2%}\")\n",
    "\n",
    "    n = int(0.9*len(all_stories))\n",
    "\n",
    "    train_data = all_stories[:n]  # use prepared splits instead, \n",
    "    val_data = all_stories[n:]  # segregate validation data by task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b29c76d-2e50-47a0-bb79-3a8d7819071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # remove y_land_idxs from return value\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # ix = torch.randint(0, len(data), (B,)) \n",
    "    ix = range(B) # HACK! for now, just use the first B stories\n",
    "    \n",
    "    x = torch.full((B, T), PAD_TOK_ID, dtype=torch.long)\n",
    "    y = torch.full((B, T), PAD_TOK_ID, dtype=torch.long)\n",
    "    x_lang_idxs = torch.full((B, T), 0, dtype=torch.long)\n",
    "    y_lang_idxs = torch.full((B, T), 0, dtype=torch.long)\n",
    "\n",
    "    for sequence_index, random_story_index in enumerate(ix):\n",
    "        story_tokens = data[random_story_index][0].long()[:T - 1]\n",
    "        story_lang_idxs = data[random_story_index][1].long()[:T - 1]\n",
    "\n",
    "        story_length = story_tokens.shape[0]\n",
    "        assert story_lang_idxs.shape == story_lang_idxs.shape\n",
    "        \n",
    "        x[sequence_index][0:story_length-1] = story_tokens[0:story_length-1]\n",
    "        y[sequence_index][0:story_length-1] = story_tokens[1:story_length]\n",
    "        x_lang_idxs[sequence_index][0: story_length-1] = story_lang_idxs[0:story_length-1]\n",
    "        y_lang_idxs[sequence_index][0: story_length-1] = story_lang_idxs[1:story_length]\n",
    "\n",
    "    return x, y, x_lang_idxs, y_lang_idxs\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    xb, yb, x_lang_idxs, y_lang_idxs = get_batch('train')\n",
    "\n",
    "\n",
    "    print(xb[0][:5])\n",
    "    print(yb[0][:5])\n",
    "    print(x_lang_idxs[0][:5])\n",
    "    print(y_lang_idxs[0][:5])\n",
    "    language = x_lang_idxs[0,0]\n",
    "    print(language)\n",
    "    # print(decode(xb[0].tolist(), language.item())) # the zero on xb is the first story. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaadce6",
   "metadata": {},
   "source": [
    "## Model defintions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3ed8ea-281b-45fa-928a-b856abc13d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    '''One Head of self-attention'''\n",
    "    def __init__(self, H):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(C, H, bias=False)\n",
    "        self.key = nn.Linear(C, H, bias=False)\n",
    "        self.value = nn.Linear(C, H, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(T, T)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        query_vectors = self.query(x)\n",
    "        key_vectors = self.key(x)\n",
    "\n",
    "\n",
    "        # Attention masking(so we can't look into the past):\n",
    "        tril = self.tril\n",
    "        wei = torch.zeros(T, T) \n",
    "        wei = wei.masked_fill(tril == 0, float('-inf')) # set the upper triangular to -inf\n",
    "\n",
    "        # multiply the two to get the attention weights\n",
    "        attention_pattern = query_vectors @ key_vectors.transpose(-2, -1) # T, T\n",
    "        attention_pattern = attention_pattern / (H ** 0.5) # scale the attention pattern for numerical stability\n",
    "        attention_weights = F.softmax(attention_pattern + wei, dim=-1) # T, T (the row dimension is the query)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        value_vectors = self.value(x) # the direction we should go in the embedding space for each token (ie more blue) T, H\n",
    "\n",
    "        # apply the attention weights to the value vectors\n",
    "        context = attention_weights @ value_vectors # T, H\n",
    "\n",
    "        # project back into original space from value space\n",
    "        return context\n",
    "\n",
    "x = torch.randn(B,T,C)\n",
    "head = Head(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1ddfac-31a8-43b6-ac7f-646dbea39fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''Multiple heads of self-attention'''\n",
    "    def __init__(self, H, C, n_heads): # H is head embedding space size, n_heads is number of heads\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(H) for _ in range(n_heads)])\n",
    "        self.combine_heads = nn.Linear(H*n_heads, C)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        x = self.combine_heads(x)  # T, C\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64c34e5-ad29-440c-ab9c-0b8c4c153087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 32])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head = MultiHeadAttention(H, C, n_heads)\n",
    "head.heads[0].forward(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0beeabe-00ec-45c8-9911-366fec57a5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    '''Feed-forward neural network'''\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(C, C * feedforward_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(C * feedforward_factor, C),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450533d3-26cb-49f0-afde-777f1b6d8e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    '''Layer normalization'''\n",
    "    def __init__(self, C, use_affine=True):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(C)) if use_affine else None\n",
    "        self.beta = nn.Parameter(torch.zeros(C)) if use_affine else None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        if self.gamma is not None and self.beta is not None:\n",
    "            return self.gamma * (x - mean) / (std + 1e-6) + self.beta\n",
    "        else:\n",
    "            return (x - mean) / (std + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b40957-1158-4e85-9b07-6a25d72338e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''Transformer block'''\n",
    "    def __init__(self, H, C, n_heads):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(H, C, n_heads)\n",
    "        self.ff = FeedForward(C)\n",
    "        self.norm1 = LayerNorm(C, use_affine=True)\n",
    "        self.norm2 = LayerNorm(C, use_affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6224a0ef-2693-4e29-b989-3e80a046bd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters in the model (millions):  12.825856\n"
     ]
    }
   ],
   "source": [
    "class MultiTokenizerGPT(nn.Module):\n",
    "    def __init__(self, n_layers, n_languages):\n",
    "        super().__init__()\n",
    "        self.token_embedding_tables = [nn.Embedding(vocab_size, C) for _ in range(n_languages)]\n",
    "        self.position_embedding_table = nn.Embedding(T, C)\n",
    "        self.lm_heads = nn.ModuleList([nn.Linear(C, vocab_size) for _ in range(n_languages)])\n",
    "        self.layers = nn.ModuleList([Block(H, C, n_heads) for _ in range(n_layers)])\n",
    "        self.block = nn.ModuleList([Block(H, C, n_heads)])\n",
    "    \n",
    "    # token_lang_idx has same shape as token_ids, encodes langs\n",
    "    def forward(self, token_ids, token_lang_idxs, targets=None):\n",
    "        B, T = token_ids.shape\n",
    "\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T))\n",
    "\n",
    "        token_emb = torch.zeros(B*T, C)\n",
    "\n",
    "        token_ids_flat = token_ids.view(-1)\n",
    "\n",
    "        unique_lang_idxs = torch.unique(token_lang_idxs)\n",
    "\n",
    "        # List of tensors, where each tensor contains the indexes in the flat tokens list of all tokens of a language.\n",
    "        # there are as many tensors as there are languages present in the batch. Total length of all tensors = # tokens\n",
    "        lang_tok_mapping = [torch.where(token_lang_idxs.view(-1) == idx) for idx in unique_lang_idxs]\n",
    "\n",
    "        for lang_id, monolingual_tokens_idx in zip(unique_lang_idxs, lang_tok_mapping):\n",
    "            lang_embedding_table = self.token_embedding_tables[lang_id]\n",
    "            monolingual_tok_emb = lang_embedding_table(token_ids_flat[monolingual_tokens_idx])\n",
    "            token_emb[monolingual_tokens_idx] = monolingual_tok_emb\n",
    "\n",
    "        x = token_emb.view(B, T, C) + pos_emb # token identities and positions contained\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x_flat = x.view(B*T, C)\n",
    "        logits = torch.zeros(B*T, vocab_size)\n",
    "\n",
    "        for lang_id, monolingual_tokens_idx in zip(unique_lang_idxs, lang_tok_mapping):\n",
    "            lm_head = self.lm_heads[lang_id]\n",
    "            new_logits = lm_head(x_flat[monolingual_tokens_idx])\n",
    "            logits[monolingual_tokens_idx] = new_logits\n",
    "\n",
    "        logits = logits.view(B,T,vocab_size)\n",
    "\n",
    "        batch_dim, sequence_dim, embedding_dim = logits.size()\n",
    "\n",
    "        if targets is None:\n",
    "            return logits, None\n",
    "        else:\n",
    "            logits_loss_view = logits.view(-1, vocab_size) \n",
    "            targets_loss_view = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits_loss_view, targets_loss_view)\n",
    "            return logits, loss\n",
    "\n",
    "        return idx\n",
    "    def prompt_model(self, prompt, max_new_tokens, language=\"spanish\", temperature=0.5):\n",
    "        token_lang_idxs = torch.full([1, T], language_id)\n",
    "\n",
    "        autoregressive_seq = [START_TOK, language_token]\n",
    "\n",
    "        autoregressive_seq += encode(prompt, language_id)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            prediction_index = len(autoregressive_seq)-1\n",
    "\n",
    "            model_input = torch.tensor(autoregressive_seq)\n",
    "            \n",
    "            while model_input.shape[0] < T:\n",
    "                pad_token = torch.tensor([PAD_TOK], dtype=torch.long)\n",
    "                model_input = torch.cat((model_input, pad_token), dim=0)\n",
    "\n",
    "            model_input = model_input.unsqueeze(0)\n",
    "\n",
    "            logits, loss = model.forward(\n",
    "                token_ids=model_input,\n",
    "                token_lang_idxs=token_lang_idxs\n",
    "                )\n",
    "            prediction_token = logits[:, prediction_index, :] / temperature\n",
    "            probabilities = F.softmax(prediction_token, dim=-1)\n",
    "            next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "            next_token = next_token.item()\n",
    "            if(next_token == END_TOK):\n",
    "                break\n",
    "\n",
    "            autoregressive_seq.append(next_token)\n",
    "        # get the autoregressive sequence\n",
    "        return decode(autoregressive_seq, language_id)\n",
    "\n",
    "\n",
    "model = MultiTokenizerGPT(n_layers, 2)\n",
    "# get the number of parameters in the model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"number of parameters in the model (millions): \", count_parameters(model) /1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e95468-ce3b-4351-9355-2859a0d52979",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    xb, yb, x_lang_idxs, y_lang_idxs = get_batch('train')\n",
    "\n",
    "    logits, loss = model(\n",
    "        token_ids = xb,\n",
    "        token_lang_idxs = x_lang_idxs,\n",
    "        targets = yb\n",
    "    )\n",
    "    print(logits.shape)\n",
    "    print(loss)\n",
    "\n",
    "    # test_idx = torch.zeros(1, T).long()\n",
    "    # model.forward(idx=test_idx)\n",
    "    # decode(model.generate(idx=test_idx, max_new_tokens=100)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d745fa-d48c-441c-886e-890c2a7636aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2684ea-3417-4708-b49d-03acf0e1dc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 10\n",
    "eval_interval = 300\n",
    "chars_per_token=3.9   # HACK!  need to compute chars per token for fair comparision\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(is_last=False):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        real_iters = eval_iters\n",
    "        if is_last and split == 'val':  # increase last eval to mitigate noise\n",
    "            real_iters *= 10 \n",
    "        losses = torch.zeros(real_iters)\n",
    "        for k in range(real_iters):\n",
    "            # X, Y = get_batch(split)\n",
    "            xb, yb, x_lang_idxs, y_lang_idxs = get_batch(split)\n",
    "\n",
    "            logits, loss = model(\n",
    "                token_ids = xb,\n",
    "                token_lang_idxs = x_lang_idxs,\n",
    "                targets = yb\n",
    "            )\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean() / chars_per_token\n",
    "    model.train()\n",
    "    return out\n",
    "    \n",
    "if TRAIN_MODEL:\n",
    "    estimate_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b37613-849e-46a5-9e67-64c432bd9ebd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    dump_model_interval = 1000\n",
    "\n",
    "    for steps in tqdm(range(0, max_iters)):\n",
    "        xb, yb, x_lang_idxs, y_lang_idxs = get_batch('train')\n",
    "        # loss\n",
    "        logits, loss = model(\n",
    "            token_ids = xb,\n",
    "            token_lang_idxs = x_lang_idxs,\n",
    "            targets = yb\n",
    "        )\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if steps % eval_interval == 0:\n",
    "            losses = estimate_loss()\n",
    "            # wandb.log({\"train\": losses['train'].item(), \"val\": losses['val'].item(), \"l2\":l2})\n",
    "            print({\"train\": losses['train'].item(), \"val\": losses['val'].item()})\n",
    "        if steps % dump_model_interval == 0 and steps > 0:\n",
    "            model_no = steps // dump_model_interval\n",
    "            torch.save(model.state_dict(), f'{MODEL_PATH}/tiny-stories-model-{model_no}.pt')\n",
    "\n",
    "    losses = estimate_loss(is_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f5ea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), f'{MODEL_PATH}/overfit-batch-1-model.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62703f61",
   "metadata": {},
   "source": [
    "## Manual prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5637b3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on cpu lazy hack activate\n"
     ]
    }
   ],
   "source": [
    "if not TRAIN_MODEL:\n",
    "    # model_file = f'{MODEL_PATH}/tiny-stories-model-18.pt'\n",
    "    model_file = f'{MODEL_PATH}/overfit-batch-1-model.pt'\n",
    "    print('on cpu lazy hack activate')\n",
    "    model.load_state_dict(torch.load(model_file,  map_location=torch.device('cpu')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b05e4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEGIN-EN][PROMPT] translate [USER] Lily went to the park.\n",
      "[BEGIN-ES]Lily fue \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[BEGIN-EN][PROMPT] translate [USER] Lily went to the park.\\n[BEGIN-ES]Lily fue  \"¡ comerla. y hacer dijo!\" jugar el y'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prompt_model(model, tokens, lang_idxs, max_new_tokens, temperature=0.5):\n",
    "    autoregressive_toks = tokens.tolist()\n",
    "    autoregressive_langs = lang_idxs.tolist()\n",
    "\n",
    "    assert len(autoregressive_toks) == len(autoregressive_langs)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        prediction_index = len(autoregressive_toks)-1\n",
    "\n",
    "        input_tokens = torch.tensor(autoregressive_toks)\n",
    "        input_langs = torch.tensor(autoregressive_langs)\n",
    "        while input_tokens.shape[0] < T:\n",
    "            input_tokens = torch.cat((input_tokens, torch.tensor([PAD_TOK_ID])))\n",
    "            input_langs = torch.cat((input_langs, torch.tensor([autoregressive_langs[-1]])))\n",
    "\n",
    "        input_tokens = input_tokens.unsqueeze(0)\n",
    "        input_langs = input_langs.unsqueeze(0)\n",
    "\n",
    "        logits, ignored_loss = model.forward(token_ids=input_tokens, token_lang_idxs=input_langs)\n",
    "        temp_scaled_logits = logits[:, prediction_index, :] / temperature\n",
    "        probabilities = F.softmax(temp_scaled_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probabilities, num_samples=1).item()\n",
    "\n",
    "        if next_token in LANG_BEGIN_TOK_IDS:\n",
    "            print('switching langs')\n",
    "            autoregressive_langs.append(LANG_BEGIN_TOK_IDS.index(next_token))\n",
    "        else:\n",
    "            autoregressive_langs.append(autoregressive_langs[-1])\n",
    "\n",
    "        autoregressive_toks.append(next_token)\n",
    "        if next_token == END_TOK_ID:\n",
    "            break\n",
    "    return autoregressive_toks, autoregressive_langs\n",
    "    # get the autoregressive sequence\n",
    "    # return decode(autoregressive_seq, language_id)\n",
    "\n",
    "# generate prompt -> hacky split -> multi encod\n",
    "prompted = (data_split.write_translation_story_tinyprompt_strs('Lily fue', 'Lily went to the park.'))\n",
    "lang_segmented = detections_to_lang_identified_text(HackyCheatingLangDetector([]).split_n_detect(prompted, END_TOKEN), prompted)\n",
    "lang_segmented = lang_segmented.removesuffix('[END]')\n",
    "\n",
    "x, lang_x = multi_tokenize_delimited_story(lang_segmented)\n",
    "print(multi_detokenize(x, lang_x))\n",
    "comp_tokens, comp_langs = prompt_model(model, x, lang_x, 10, temperature=.01)\n",
    "multi_detokenize(comp_tokens, comp_langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71a0ceb-ce02-4910-b814-68b54c0927c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256])\n",
      "tensor([  5,   0, 389, 226,   1])\n"
     ]
    }
   ],
   "source": [
    "xb, xl, yb, yl = get_batch('train')\n",
    "print(xb.shape)\n",
    "print(xb[0][:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf25a21",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
